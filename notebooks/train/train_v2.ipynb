{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import getModelName, getNumberOfSpeakers, Accuracy, scoreCosineDistance, chkptsave, Score\n",
    "from data import Dataset, normalizeFeatures, featureReader\n",
    "from model import SpeakerClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Input params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "execute and copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_data_dir': '/scratch/speaker_databases/', 'valid_data_dir': '/scratch/speaker_databases/VoxCeleb-1/wav', 'train_labels_path': 'labels/Vox2.ndx', 'data_mode': 'normal', 'valid_clients': 'labels/clients.ndx', 'valid_impostors': 'labels/impostors.ndx', 'out_dir': './models/model1', 'model_name': 'CNN', 'front_end': 'VGG4L', 'window_size': 3.5, 'randomSlicing': False, 'normalization': 'cmn', 'kernel_size': 1024, 'embedding_size': 400, 'heads_number': 32, 'pooling_method': 'DoubleMHA', 'mask_prob': 0.3, 'scalingFactor': 30.0, 'marginFactor': 0.4, 'annealing': False, 'optimizer': 'Adam', 'learning_rate': 0.0001, 'weight_decay': 0.001, 'batch_size': 64, 'gradientAccumulation': 2, 'max_epochs': 1000000, 'early_stopping': 25, 'print_every': 1000, 'requeue': False, 'validate_every': 10000, 'num_workers': 2}\r\n"
     ]
    }
   ],
   "source": [
    "! python args_input_simulation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "    #'train_data_dir': '/home/usuaris/veu/federico.costa/datasets/voxceleb2/dev',\n",
    "    'train_data_dir': '',\n",
    "    'valid_data_dir': '', \n",
    "    'train_labels_path': '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/files_directories/labels/labels.ndx', \n",
    "    'data_mode': 'normal', \n",
    "    'valid_clients': '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/files_directories/labels/clients.ndx', \n",
    "    'valid_impostors': '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/files_directories/labels/impostors.ndx', \n",
    "    'out_dir': './models/model1', \n",
    "    'model_name': 'CNN', \n",
    "    'front_end': 'VGG4L', \n",
    "    'window_size': 3.5, \n",
    "    'randomSlicing': False, \n",
    "    'normalization': 'cmn', \n",
    "    'kernel_size': 1024, \n",
    "    'embedding_size': 400, \n",
    "    'heads_number': 32, \n",
    "    'pooling_method': 'DoubleMHA', \n",
    "    'mask_prob': 0.3, \n",
    "    'scalingFactor': 30.0, \n",
    "    'marginFactor': 0.4, \n",
    "    'annealing': False, \n",
    "    'optimizer': 'Adam', \n",
    "    'learning_rate': 0.0001, \n",
    "    'weight_decay': 0.001, \n",
    "    'batch_size': 64, \n",
    "    'gradientAccumulation': 2, \n",
    "    'max_epochs': 1000000, \n",
    "    'early_stopping': 25, \n",
    "    'print_every': 1000, \n",
    "    'requeue': False, \n",
    "    'validate_every': 10000, \n",
    "    'num_workers': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, params, device):\n",
    "        \n",
    "        self.params = params\n",
    "        \n",
    "        self.params.max_epochs = 2\n",
    "        self.params.batch_size = 64\n",
    "        self.params.validate_every = 7\n",
    "        self.params.print_every = 1\n",
    "        #self.params.num_workers = 1\n",
    "        \n",
    "        self.device = device\n",
    "        self.__load_network()\n",
    "        self.__load_data()\n",
    "        self.__load_optimizer()\n",
    "        self.__load_criterion()\n",
    "        self.__initialize_training_variables()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __load_previous_states(self):\n",
    "\n",
    "        list_files = os.listdir(self.params.out_dir)\n",
    "        list_files = [self.params.out_dir + '/' + f for f in list_files if '.chkpt' in f]\n",
    "        if list_files:\n",
    "            file2load = max(list_files, key=os.path.getctime)\n",
    "            checkpoint = torch.load(file2load, map_location=self.device)\n",
    "            try:\n",
    "                self.net.load_state_dict(checkpoint['model'])\n",
    "            except RuntimeError:\n",
    "                self.net.module.load_state_dict(checkpoint['model'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            self.params = checkpoint['settings']\n",
    "            self.starting_epoch = checkpoint['epoch']+1\n",
    "            self.step = checkpoint['step']\n",
    "            print('Model \"%s\" is Loaded for requeue process' % file2load)\n",
    "        else:\n",
    "            self.step = 0\n",
    "            self.starting_epoch = 1\n",
    "\n",
    "    def __initialize_training_variables(self):\n",
    "\n",
    "        if self.params.requeue:\n",
    "            self.__load_previous_states()\n",
    "        else:\n",
    "            self.step = 0\n",
    "            self.starting_epoch = 0\n",
    "\n",
    "        self.best_EER = 50.0\n",
    "        self.stopping = 0.0\n",
    "\n",
    "\n",
    "    def __load_network(self):\n",
    "\n",
    "        self.net = SpeakerClassifier(self.params, self.device)\n",
    "        self.net.to(self.device)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            self.net = nn.DataParallel(self.net)\n",
    "\n",
    "\n",
    "    def __load_data(self):\n",
    "        print('Loading Data and Labels')\n",
    "        with open(self.params.train_labels_path, 'r') as data_labels_file:\n",
    "            train_labels=data_labels_file.readlines()\n",
    "\n",
    "        data_loader_parameters = {'batch_size': self.params.batch_size, 'shuffle': True, 'num_workers': self.params.num_workers}\n",
    "        self.training_generator = DataLoader(Dataset(train_labels, self.params), **data_loader_parameters)\n",
    "\n",
    "\n",
    "    def __load_optimizer(self):\n",
    "        if self.params.optimizer == 'Adam':\n",
    "            self.optimizer = optim.Adam(self.net.parameters(), lr=self.params.learning_rate, weight_decay=self.params.weight_decay)\n",
    "        if self.params.optimizer == 'SGD':\n",
    "            self.optimizer = optim.SGD(self.net.parameters(), lr=self.params.learning_rate, weight_decay=self.params.weight_decay)\n",
    "        if self.params.optimizer == 'RMSprop':\n",
    "            self.optimizer = optim.RMSprop(self.net.parameters(), lr=self.params.learning_rate, weight_decay=self.params.weight_decay)\n",
    "\n",
    "    def __update_optimizer(self):\n",
    "\n",
    "        if self.params.optimizer == 'SGD' or self.params.optimizer == 'Adam':\n",
    "            for paramGroup in self.optimizer.param_groups:\n",
    "                paramGroup['lr'] *= 0.5\n",
    "            print('New Learning Rate: {}'.format(paramGroup['lr']))\n",
    "    \n",
    "    def __load_criterion(self):\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def __initialize_batch_variables(self):\n",
    "\n",
    "        self.print_time = time.time()\n",
    "        self.train_loss = 0.0\n",
    "        self.train_accuracy = 0.0\n",
    "        self.train_batch = 0\n",
    "\n",
    "    def __extractInputFromFeature(self, sline):\n",
    "\n",
    "        #features1 = normalizeFeatures(featureReader(self.params.valid_data_dir + '/' + sline[0] + '.pickle'), normalization=self.params.normalization)\n",
    "        #features2 = normalizeFeatures(featureReader(self.params.valid_data_dir + '/' + sline[1] + '.pickle'), normalization=self.params.normalization)\n",
    "        features1 = normalizeFeatures(featureReader(self.params.valid_data_dir  + sline[0] + '.pickle'), normalization=self.params.normalization)\n",
    "        features2 = normalizeFeatures(featureReader(self.params.valid_data_dir  + sline[1] + '.pickle'), normalization=self.params.normalization)\n",
    "\n",
    "        \n",
    "        input1 = torch.FloatTensor(features1).to(self.device)\n",
    "        input2 = torch.FloatTensor(features2).to(self.device)\n",
    "        \n",
    "        return input1.unsqueeze(0), input2.unsqueeze(0)\n",
    "\n",
    "    def extract_scores(self, trials):\n",
    "\n",
    "        scores = []\n",
    "        for line in trials:\n",
    "            sline = line[:-1].split()\n",
    "\n",
    "            input1, input2 = self.__extractInputFromFeature(sline)\n",
    "\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                emb1, emb2 = self.net.module.getEmbedding(input1), self.net.module.getEmbedding(input2)\n",
    "            else:\n",
    "                emb1, emb2 = self.net.getEmbedding(input1), self.net.getEmbedding(input2)\n",
    "\n",
    "            dist = scoreCosineDistance(emb1, emb2)\n",
    "            scores.append(dist.item())\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def calculate_EER(self, CL, IM):\n",
    "\n",
    "        thresholds = np.arange(-1,1,0.01)\n",
    "        FRR, FAR = np.zeros(len(thresholds)), np.zeros(len(thresholds))\n",
    "        for idx,th in enumerate(thresholds):\n",
    "            FRR[idx] = Score(CL, th,'FRR')\n",
    "            FAR[idx] = Score(IM, th,'FAR')\n",
    "\n",
    "        EER_Idx = np.argwhere(np.diff(np.sign(FAR - FRR)) != 0).reshape(-1)\n",
    "        if len(EER_Idx)>0:\n",
    "            if len(EER_Idx)>1:\n",
    "                EER_Idx = EER_Idx[0]\n",
    "            EER = round((FAR[int(EER_Idx)] + FRR[int(EER_Idx)])/2,4)\n",
    "        else:\n",
    "            EER = 50.00\n",
    "        return EER\n",
    "\n",
    "    def __getAnnealedFactor(self):\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            return self.net.module.predictionLayer.getAnnealedFactor(self.step)\n",
    "        else:\n",
    "            return self.net.predictionLayer.getAnnealedFactor(self.step)\n",
    "\n",
    "    def __validate(self):\n",
    "        \n",
    "        print(\"        Using __validate!\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            valid_time = time.time()\n",
    "            self.net.eval()\n",
    "            # EER Validation\n",
    "            with open(params.valid_clients,'r') as clients_in, open(params.valid_impostors,'r') as impostors_in:\n",
    "                # score clients\n",
    "                CL = self.extract_scores(clients_in)\n",
    "                IM = self.extract_scores(impostors_in)\n",
    "            # Compute EER\n",
    "            EER = self.calculate_EER(CL, IM)\n",
    "            \n",
    "            annealedFactor = self.__getAnnealedFactor()\n",
    "            print('Annealed Factor is {}.'.format(annealedFactor))\n",
    "            print('--Validation Epoch:{epoch: d}, Updates:{Num_Batch: d}, EER:{eer: 3.3f}, elapse:{elapse: 3.3f} min'.format(epoch=self.epoch, Num_Batch=self.step, eer=EER, elapse=(time.time()-valid_time)/60))\n",
    "            # early stopping and save the best model\n",
    "            if EER < self.best_EER:\n",
    "                self.best_EER = EER\n",
    "                self.stopping = 0\n",
    "                print('We found a better model!')\n",
    "                chkptsave(params, self.net, self.optimizer, self.epoch, self.step)\n",
    "            else:\n",
    "                self.stopping += 1\n",
    "                print('Better Accuracy is: {}. {} epochs of no improvement'.format(self.best_EER, self.stopping))\n",
    "            self.print_time = time.time()\n",
    "            self.net.train()\n",
    "\n",
    "    def __update(self):\n",
    "        \n",
    "        print(\"        Using __update!\")\n",
    "\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.step += 1\n",
    "\n",
    "        if self.step % int(self.params.print_every) == 0:\n",
    "            print('        Training Epoch:{epoch: d}, Updates:{Num_Batch: d} -----> xent:{xnet: .3f}, Accuracy:{acc: .2f}, elapse:{elapse: 3.3f} min'.format(epoch=self.epoch, Num_Batch=self.step, xnet=self.train_loss / self.train_batch, acc=self.train_accuracy *100/ self.train_batch, elapse=(time.time()-self.print_time)/60))\n",
    "            self.__initialize_batch_variables()\n",
    "\n",
    "        # validation\n",
    "        if self.step % self.params.validate_every == 0:\n",
    "            self.__validate()\n",
    "\n",
    "    def __updateTrainningVariables(self):\n",
    "\n",
    "        if (self.stopping+1)% 15 ==0:\n",
    "            self.__update_optimizer()\n",
    "\n",
    "    def __randomSlice(self, inputTensor):\n",
    "        index = random.randrange(200,self.params.window_size*100)\n",
    "        return inputTensor[:,:index,:]\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        print('Start Training')\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        # Loop over epochs\n",
    "        for self.epoch in range(self.starting_epoch, self.params.max_epochs):  # loop over the dataset multiple times\n",
    "            print(f\"Epoch: {self.epoch}\")\n",
    "            \n",
    "            self.net.train()\n",
    "            \n",
    "            # Initialize batch variables\n",
    "            self.__initialize_batch_variables()\n",
    "            print(f\"    train_loss: {self.train_loss}\")\n",
    "            print(f\"    train_accuracy: {self.train_accuracy}\")\n",
    "            print(f\"    train_batch: {self.train_batch}\")\n",
    "            \n",
    "            # Iterate over batchs\n",
    "            for input, label in self.training_generator:\n",
    "                \n",
    "                print(f\"    Step: {self.step}\")\n",
    "                \n",
    "                # input are the batch_size spectrograms for each label\n",
    "                # label are the batch_size labels\n",
    "                input, label = input.float().to(self.device), label.long().to(self.device)\n",
    "                input = self.__randomSlice(input) if self.params.randomSlicing else input\n",
    "                #print(f\"        input: {input.size()}\")\n",
    "                #print(f\"        label: {label}\")\n",
    "                \n",
    "                \n",
    "                prediction, AMPrediction  = self.net(input, label=label, step=self.step)\n",
    "                #print(f\"        prediction: {prediction}\")\n",
    "                #print(f\"        AMPrediction: {AMPrediction.size()}\")\n",
    "                \n",
    "                self.fede_prediction = prediction\n",
    "                self.fede_label = label\n",
    "                self.fede_accuracy = Accuracy(prediction, label)\n",
    "                \n",
    "                \n",
    "                loss = self.criterion(AMPrediction, label)\n",
    "                loss.backward()\n",
    "                self.train_accuracy += Accuracy(prediction, label)\n",
    "                self.train_loss += loss.item()\n",
    "                \n",
    "                self.train_batch += 1\n",
    "                \n",
    "                print(f\"        train_loss: {self.train_loss}\")\n",
    "                print(f\"        train_accuracy: {self.train_accuracy}\")\n",
    "                print(f\"        train_batch: {self.train_batch}\")\n",
    "                print(f\"        gradientAccumulation: {self.params.gradientAccumulation}\")\n",
    "\n",
    "                if self.train_batch % self.params.gradientAccumulation == 0:\n",
    "                    self.__update()\n",
    "\n",
    "            if self.stopping > self.params.early_stopping:\n",
    "                print('    --Best Model EER%%: %.2f' %(self.best_EER))\n",
    "                break\n",
    "            \n",
    "            self.__updateTrainningVariables()\n",
    "        \n",
    "            print(\"-\"*50)\n",
    "            \n",
    "                #print(f\"label: {self.label}\")\n",
    "                #print(f\"prediction: {prediction}\")\n",
    "                #print(f\"AMPrediction: {AMPrediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Speaker Labels\n",
      "Defining Device...\n",
      "device: cuda:0\n",
      "Loading Trainer\n",
      "Loading Data and Labels\n",
      "Start Training\n",
      "--------------------------------------------------\n",
      "Epoch: 0\n",
      "    train_loss: 0.0\n",
      "    train_accuracy: 0.0\n",
      "    train_batch: 0\n",
      "    Step: 0\n",
      "        train_loss: 13.552813529968262\n",
      "        train_accuracy: 0.171875\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 0\n",
      "        train_loss: 26.93877124786377\n",
      "        train_accuracy: 0.390625\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 0, Updates: 1 -----> xent: 13.469, Accuracy: 19.53, elapse: 0.027 min\n",
      "    Step: 1\n",
      "        train_loss: 13.586601257324219\n",
      "        train_accuracy: 0.28125\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 1\n",
      "        train_loss: 27.047398567199707\n",
      "        train_accuracy: 0.546875\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 0, Updates: 2 -----> xent: 13.524, Accuracy: 27.34, elapse: 0.019 min\n",
      "    Step: 2\n",
      "        train_loss: 12.565530776977539\n",
      "        train_accuracy: 0.46875\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 2\n",
      "        train_loss: 25.260903358459473\n",
      "        train_accuracy: 0.859375\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 0, Updates: 3 -----> xent: 12.630, Accuracy: 42.97, elapse: 0.020 min\n",
      "    Step: 3\n",
      "        train_loss: 12.297975540161133\n",
      "        train_accuracy: 0.578125\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 3\n",
      "        train_loss: 24.426701545715332\n",
      "        train_accuracy: 1.125\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 0, Updates: 4 -----> xent: 12.213, Accuracy: 56.25, elapse: 0.020 min\n",
      "    Step: 4\n",
      "        train_loss: 10.021332740783691\n",
      "        train_accuracy: 0.859375\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 4\n",
      "        train_loss: 20.174072265625\n",
      "        train_accuracy: 1.671875\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 0, Updates: 5 -----> xent: 10.087, Accuracy: 83.59, elapse: 0.020 min\n",
      "    Step: 5\n",
      "        train_loss: 8.383047103881836\n",
      "        train_accuracy: 0.875\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 5\n",
      "        train_loss: 16.459203720092773\n",
      "        train_accuracy: 1.78125\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 0, Updates: 6 -----> xent: 8.230, Accuracy: 89.06, elapse: 0.020 min\n",
      "    Step: 6\n",
      "        train_loss: 6.3950371742248535\n",
      "        train_accuracy: 0.90625\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 6\n",
      "        train_loss: 13.251438617706299\n",
      "        train_accuracy: 1.78125\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 0, Updates: 7 -----> xent: 6.626, Accuracy: 89.06, elapse: 0.020 min\n",
      "        Using __validate!\n",
      "Annealed Factor is 1.0.\n",
      "--Validation Epoch: 0, Updates: 7, EER: 50.000, elapse: 0.005 min\n",
      "Better Accuracy is: 50.0. 1.0 epochs of no improvement\n",
      "    Step: 7\n",
      "        train_loss: 3.8122944831848145\n",
      "        train_accuracy: 1.0\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "--------------------------------------------------\n",
      "Epoch: 1\n",
      "    train_loss: 0.0\n",
      "    train_accuracy: 0.0\n",
      "    train_batch: 0\n",
      "    Step: 7\n",
      "        train_loss: 5.0781121253967285\n",
      "        train_accuracy: 0.953125\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 7\n",
      "        train_loss: 10.59160852432251\n",
      "        train_accuracy: 1.859375\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 1, Updates: 8 -----> xent: 5.296, Accuracy: 92.97, elapse: 0.026 min\n",
      "    Step: 8\n",
      "        train_loss: 4.753661155700684\n",
      "        train_accuracy: 0.921875\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 8\n",
      "        train_loss: 8.606644868850708\n",
      "        train_accuracy: 1.859375\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 1, Updates: 9 -----> xent: 4.303, Accuracy: 92.97, elapse: 0.020 min\n",
      "    Step: 9\n",
      "        train_loss: 3.4274041652679443\n",
      "        train_accuracy: 0.9375\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 9\n",
      "        train_loss: 7.350009441375732\n",
      "        train_accuracy: 1.890625\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 1, Updates: 10 -----> xent: 3.675, Accuracy: 94.53, elapse: 0.020 min\n",
      "    Step: 10\n",
      "        train_loss: 3.2490832805633545\n",
      "        train_accuracy: 0.921875\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 10\n",
      "        train_loss: 5.99729323387146\n",
      "        train_accuracy: 1.859375\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 1, Updates: 11 -----> xent: 2.999, Accuracy: 92.97, elapse: 0.020 min\n",
      "    Step: 11\n",
      "        train_loss: 3.419405698776245\n",
      "        train_accuracy: 0.890625\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 11\n",
      "        train_loss: 6.503273010253906\n",
      "        train_accuracy: 1.796875\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 1, Updates: 12 -----> xent: 3.252, Accuracy: 89.84, elapse: 0.020 min\n",
      "    Step: 12\n",
      "        train_loss: 3.2812116146087646\n",
      "        train_accuracy: 0.890625\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 12\n",
      "        train_loss: 6.381895303726196\n",
      "        train_accuracy: 1.78125\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 1, Updates: 13 -----> xent: 3.191, Accuracy: 89.06, elapse: 0.020 min\n",
      "    Step: 13\n",
      "        train_loss: 3.230435848236084\n",
      "        train_accuracy: 0.890625\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "    Step: 13\n",
      "        train_loss: 5.655432462692261\n",
      "        train_accuracy: 1.828125\n",
      "        train_batch: 2\n",
      "        gradientAccumulation: 2\n",
      "        Using __update!\n",
      "        Training Epoch: 1, Updates: 14 -----> xent: 2.828, Accuracy: 91.41, elapse: 0.020 min\n",
      "        Using __validate!\n",
      "Annealed Factor is 1.0.\n",
      "--Validation Epoch: 1, Updates: 14, EER: 5.000, elapse: 0.005 min\n",
      "We found a better model!\n",
      "Saving checkpoint in: ./models/model1/CNN_VGG4L_3.5_128batchSize_0.0001lr_0.001weightDecay_1024kernel_400embSize_30.0s_0.4m_DoubleMHA_32_14.chkpt\n",
      "    Step: 14\n",
      "        train_loss: 3.6160080432891846\n",
      "        train_accuracy: 0.9\n",
      "        train_batch: 1\n",
      "        gradientAccumulation: 2\n",
      "--------------------------------------------------\n",
      "CPU times: user 13.3 s, sys: 5.13 s, total: 18.5 s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# PARAMS\n",
    "\n",
    "params = argparse.Namespace(**params_dict)\n",
    "\n",
    "params.model_name = getModelName(params)\n",
    "params.num_spkrs = getNumberOfSpeakers(params.train_labels_path) \n",
    "\n",
    "print(f\"{params.num_spkrs} Speaker Labels\")\n",
    "\n",
    "if not os.path.exists(params.out_dir):\n",
    "    os.makedirs(params.out_dir)\n",
    "    print(f\"Output folder created at {params.out_dir}\")\n",
    "    \n",
    "if False:\n",
    "    with open(params.out_dir + '/' + params.model_name + '_config.pkl', 'wb') as handle:\n",
    "        pickle.dump(params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"{params.out_dir + '/' + params.model_name + '_config.pkl'}\")\n",
    "\n",
    "# MAIN\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "    \n",
    "print('Defining Device...')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "\n",
    "print('Loading Trainer')\n",
    "trainer = Trainer(params, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting clients from /home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/files_directories/labels/test_clients.ndx\n",
      "Getting impostors from /home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/files_directories/labels/test_impostors.ndx\n",
      "CPU times: user 10min 16s, sys: 2min 46s, total: 13min 3s\n",
      "Wall time: 13min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with torch.no_grad():\n",
    "    trainer.net.eval()\n",
    "    \n",
    "    # EER Validation\n",
    "    \n",
    "    path_clients = '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/files_directories/labels/test_clients.ndx'\n",
    "    path_impostors = '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/files_directories/labels/test_impostors.ndx'\n",
    "    \n",
    "    \n",
    "    print(f\"Getting clients from {path_clients}\")\n",
    "    print(f\"Getting impostors from {path_impostors}\")\n",
    "    \n",
    "    with open(path_clients,'r') as clients_in, open(path_impostors,'r') as impostors_in:\n",
    "        \n",
    "        # score clients\n",
    "        CL = trainer.extract_scores(clients_in)\n",
    "        IM = trainer.extract_scores(impostors_in)\n",
    "        \n",
    "        # Compute EER\n",
    "        EER = trainer.calculate_EER(CL, IM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.0455"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.2392575889825821,\n",
       " -0.09197711944580078,\n",
       " 0.45905572175979614,\n",
       " 0.6113088726997375,\n",
       " 0.46484145522117615,\n",
       " 0.9021791219711304,\n",
       " 0.531754732131958,\n",
       " 0.6643937230110168,\n",
       " 0.5130282640457153,\n",
       " 0.44185400009155273,\n",
       " 0.40938806533813477,\n",
       " 0.8174954056739807,\n",
       " 0.03786841407418251,\n",
       " 0.9692579507827759,\n",
       " -0.43769729137420654,\n",
       " 0.929316520690918,\n",
       " -0.36475878953933716,\n",
       " -0.4534321427345276,\n",
       " 0.9874919056892395,\n",
       " 0.880779504776001,\n",
       " 0.15388286113739014,\n",
       " 0.8952730894088745,\n",
       " -0.136513352394104,\n",
       " -0.25249144434928894,\n",
       " 0.9355823993682861,\n",
       " -0.29948920011520386,\n",
       " -0.32106247544288635,\n",
       " -0.10224729776382446,\n",
       " 0.5268607139587402,\n",
       " 0.9011231660842896,\n",
       " 0.3721476197242737,\n",
       " 0.46108657121658325,\n",
       " 0.6554352045059204,\n",
       " 0.22522985935211182,\n",
       " 0.7701794505119324,\n",
       " 0.23442678153514862,\n",
       " 0.9642038345336914,\n",
       " 0.7734853029251099,\n",
       " -0.2934712767601013,\n",
       " 0.1288439929485321,\n",
       " -0.24065382778644562,\n",
       " -0.4183798134326935,\n",
       " 0.9548529982566833,\n",
       " -0.4304813742637634,\n",
       " 0.6675812602043152,\n",
       " -0.050593748688697815,\n",
       " 0.9548270106315613,\n",
       " 0.04765696078538895,\n",
       " 0.0863291323184967,\n",
       " 0.6749953031539917,\n",
       " -0.029903162270784378,\n",
       " -0.18502569198608398,\n",
       " 0.4777863621711731,\n",
       " 0.7541623115539551,\n",
       " 0.5099290013313293,\n",
       " 0.4667174220085144,\n",
       " -0.008499696850776672,\n",
       " 0.7164716720581055,\n",
       " 0.3383055329322815,\n",
       " 0.3028486371040344,\n",
       " 0.8397982716560364,\n",
       " 0.8966655731201172,\n",
       " -0.2840213477611542,\n",
       " 0.8794934749603271,\n",
       " 0.7541488409042358,\n",
       " 0.32208746671676636,\n",
       " 0.6563618183135986,\n",
       " 0.4047011137008667,\n",
       " 0.8939536213874817,\n",
       " -0.4143683612346649,\n",
       " -0.3812832832336426,\n",
       " 0.004452327266335487,\n",
       " 0.2671666145324707,\n",
       " 0.2177819013595581,\n",
       " 0.5635614395141602,\n",
       " 0.603848397731781,\n",
       " -0.3781607151031494,\n",
       " 0.8456296324729919,\n",
       " -0.43858981132507324,\n",
       " -0.2533818483352661,\n",
       " -0.3627588152885437,\n",
       " 0.9337478876113892,\n",
       " -0.16095399856567383,\n",
       " 0.650368869304657,\n",
       " -0.0394073948264122,\n",
       " 0.7214298248291016,\n",
       " 0.9287123680114746,\n",
       " 0.12099327147006989,\n",
       " 0.9768331050872803,\n",
       " 0.29168587923049927,\n",
       " -0.38292524218559265,\n",
       " -0.21238288283348083,\n",
       " -0.1951247602701187,\n",
       " -0.3134724795818329,\n",
       " 0.859932005405426,\n",
       " -0.26827695965766907,\n",
       " 0.9843948483467102,\n",
       " 0.939996063709259,\n",
       " 0.9762254953384399,\n",
       " -0.3417419493198395,\n",
       " -0.08389653265476227,\n",
       " 0.8523718118667603,\n",
       " -0.13849419355392456,\n",
       " 0.21176700294017792,\n",
       " 0.6023480892181396,\n",
       " -0.32555723190307617,\n",
       " -0.06704898923635483,\n",
       " 0.09307738393545151,\n",
       " 0.929998517036438,\n",
       " -0.1069052517414093,\n",
       " -0.07399602979421616,\n",
       " 0.9135099649429321,\n",
       " 0.5395500659942627,\n",
       " -0.21019309759140015,\n",
       " 0.8430448174476624,\n",
       " -0.17693579196929932,\n",
       " -0.19602876901626587,\n",
       " -0.4410136938095093,\n",
       " -0.3033566474914551,\n",
       " -0.18448561429977417,\n",
       " -0.09481233358383179,\n",
       " 0.9012513756752014,\n",
       " -0.18493834137916565,\n",
       " -0.34312736988067627,\n",
       " 0.9921482801437378,\n",
       " 0.9764320254325867,\n",
       " -0.4951416254043579,\n",
       " -0.5308519005775452,\n",
       " 0.3898237943649292,\n",
       " 0.4034156799316406,\n",
       " 0.9498984813690186,\n",
       " 0.327098548412323,\n",
       " 0.8734478950500488,\n",
       " 0.07685645669698715,\n",
       " 0.883953869342804,\n",
       " 0.17304769158363342,\n",
       " 0.3560613989830017,\n",
       " 0.9656311869621277,\n",
       " 0.8344600200653076,\n",
       " 0.5230778455734253,\n",
       " -0.18207904696464539,\n",
       " -0.16755789518356323,\n",
       " -0.23961880803108215,\n",
       " 0.34836021065711975,\n",
       " -0.2281465381383896,\n",
       " -0.13840706646442413,\n",
       " -0.07645746320486069,\n",
       " -0.22645124793052673,\n",
       " 0.18632079660892487,\n",
       " 0.8029444813728333,\n",
       " 0.0854368731379509,\n",
       " 0.8254328966140747,\n",
       " 0.4554131329059601,\n",
       " 0.8574049472808838,\n",
       " 0.42595481872558594,\n",
       " 0.615527868270874,\n",
       " 0.466424822807312,\n",
       " 0.5317019820213318,\n",
       " 0.8833547830581665,\n",
       " 0.9695295095443726,\n",
       " 0.03238530084490776,\n",
       " 0.021673018112778664,\n",
       " 0.7430696487426758,\n",
       " -0.03573489934206009,\n",
       " 0.8849433064460754,\n",
       " -0.4108975827693939,\n",
       " 0.9759021997451782,\n",
       " 0.9919027090072632,\n",
       " -0.05085412412881851,\n",
       " 0.9896042346954346,\n",
       " -0.6285532116889954,\n",
       " -0.5711659789085388,\n",
       " 0.9477734565734863,\n",
       " 0.7318330407142639,\n",
       " 0.9730144739151001,\n",
       " -0.2828388512134552,\n",
       " 0.9981616139411926,\n",
       " -0.45798826217651367,\n",
       " 0.983351469039917,\n",
       " -0.49363186955451965,\n",
       " -0.16314932703971863,\n",
       " 0.7944904565811157,\n",
       " -0.29235365986824036,\n",
       " 0.8334770202636719,\n",
       " 0.9432656168937683,\n",
       " -0.4348943531513214,\n",
       " 0.9182703495025635,\n",
       " -0.491272896528244,\n",
       " 0.6162440180778503,\n",
       " -0.623037576675415,\n",
       " 0.19715049862861633,\n",
       " -0.5531067848205566,\n",
       " -0.1935618817806244,\n",
       " -0.5641921758651733,\n",
       " 0.9749507904052734,\n",
       " -0.3956329822540283,\n",
       " 0.07057332992553711,\n",
       " -0.4401093125343323,\n",
       " -0.5066550970077515,\n",
       " -0.15256571769714355,\n",
       " 0.897133469581604,\n",
       " -0.3831363320350647,\n",
       " 0.9065119028091431,\n",
       " -0.2661200165748596,\n",
       " 0.9890738725662231,\n",
       " -0.2580946683883667,\n",
       " -0.38363832235336304,\n",
       " -0.02516566775739193,\n",
       " 0.04436105117201805,\n",
       " -0.01816432550549507,\n",
       " 0.9759451746940613,\n",
       " -0.33052965998649597,\n",
       " 0.45770472288131714,\n",
       " 0.49949097633361816,\n",
       " 0.39310723543167114,\n",
       " 0.286801815032959,\n",
       " 0.21934720873832703,\n",
       " 0.6761168241500854,\n",
       " 0.39371275901794434,\n",
       " 0.021521706134080887,\n",
       " 0.8925119042396545,\n",
       " 0.9832971096038818,\n",
       " -0.3783513903617859,\n",
       " 0.853056013584137,\n",
       " -0.5711115598678589,\n",
       " 0.971971869468689,\n",
       " -0.4701422154903412,\n",
       " 0.9954712986946106,\n",
       " 0.3775961995124817,\n",
       " 0.5348001718521118,\n",
       " 0.45037299394607544,\n",
       " 0.41037219762802124,\n",
       " 0.19238457083702087,\n",
       " 0.15287259221076965,\n",
       " -0.28563860058784485,\n",
       " 0.46567288041114807,\n",
       " 0.9190136194229126,\n",
       " 0.9093202948570251,\n",
       " -0.059778034687042236,\n",
       " -0.1926405131816864,\n",
       " 0.9849013090133667,\n",
       " -0.6300232410430908,\n",
       " -0.48244595527648926,\n",
       " 0.15032123029232025,\n",
       " 0.9191918969154358,\n",
       " -0.46398141980171204,\n",
       " 0.8098234534263611,\n",
       " 0.40398740768432617,\n",
       " 0.2144467979669571,\n",
       " 0.9214088916778564,\n",
       " 0.8335895538330078,\n",
       " 0.8502213954925537,\n",
       " -0.04434911534190178,\n",
       " -0.3749053180217743,\n",
       " 0.31537261605262756,\n",
       " 0.6474276781082153,\n",
       " 0.9852643609046936,\n",
       " 0.9946123361587524,\n",
       " 0.9852814078330994,\n",
       " -0.3792528212070465,\n",
       " -0.4959147572517395,\n",
       " 0.8527835607528687,\n",
       " -0.37960803508758545,\n",
       " 0.6395171880722046,\n",
       " 0.5708063840866089,\n",
       " -0.48901090025901794,\n",
       " -0.08540281653404236,\n",
       " 0.6209052205085754,\n",
       " -0.3842155337333679,\n",
       " 0.9475128650665283,\n",
       " 0.9680256843566895,\n",
       " 0.08851310610771179,\n",
       " 0.8561937808990479,\n",
       " 0.9073244333267212,\n",
       " -0.5755679607391357,\n",
       " 0.9963302612304688,\n",
       " 0.07294024527072906,\n",
       " -0.1866687834262848,\n",
       " 0.2017303705215454,\n",
       " 0.5478025674819946,\n",
       " 0.0114110317081213,\n",
       " -0.011770997196435928,\n",
       " -0.16063851118087769,\n",
       " 0.6623590588569641,\n",
       " 0.914192795753479,\n",
       " 0.898158848285675,\n",
       " 0.3335012197494507,\n",
       " 0.8800958395004272,\n",
       " 0.9434386491775513,\n",
       " -0.24656805396080017,\n",
       " -0.19889436662197113,\n",
       " 0.8255922794342041,\n",
       " -0.5263702273368835,\n",
       " 0.9976771473884583,\n",
       " 0.10418456792831421,\n",
       " -0.4989258944988251,\n",
       " -0.09425859898328781,\n",
       " -0.3078514337539673,\n",
       " -0.2501395344734192,\n",
       " 0.9526613354682922,\n",
       " 0.7571201324462891,\n",
       " 0.8366748094558716,\n",
       " 0.8072556257247925,\n",
       " 0.5775566101074219,\n",
       " -0.35373786091804504,\n",
       " -0.2816953659057617,\n",
       " 0.4911895990371704,\n",
       " 0.5950415730476379,\n",
       " -0.31138455867767334,\n",
       " 0.8907090425491333,\n",
       " -0.28458672761917114,\n",
       " 0.7952471375465393,\n",
       " 0.9135100841522217,\n",
       " -0.48838305473327637,\n",
       " -0.2545626163482666,\n",
       " -0.5239613056182861,\n",
       " 0.8860243558883667,\n",
       " -0.018250172957777977,\n",
       " -0.322609543800354,\n",
       " -0.2868186831474304,\n",
       " 0.3003743886947632,\n",
       " -0.5224388241767883,\n",
       " -0.39420241117477417,\n",
       " -0.5399696826934814,\n",
       " -0.49489033222198486,\n",
       " -0.5873048305511475,\n",
       " 0.9902787208557129,\n",
       " -0.6244004964828491,\n",
       " 0.9379254579544067,\n",
       " -0.3856690526008606,\n",
       " 0.9298492670059204,\n",
       " 0.9111053943634033,\n",
       " 0.9953827857971191,\n",
       " -0.5551743507385254,\n",
       " -0.46884769201278687,\n",
       " 0.9697080254554749,\n",
       " 0.8832784295082092,\n",
       " 0.59843909740448,\n",
       " -0.3546897768974304,\n",
       " -0.41305863857269287,\n",
       " -0.04980620741844177,\n",
       " 0.9502102136611938,\n",
       " -0.5325753688812256,\n",
       " 0.9399280548095703,\n",
       " 0.9979709386825562,\n",
       " -0.2394816279411316,\n",
       " -0.5189307332038879,\n",
       " -0.592966616153717,\n",
       " 0.3354588449001312,\n",
       " 0.5810830593109131,\n",
       " 0.8284976482391357,\n",
       " 0.4577798545360565,\n",
       " 0.89665687084198,\n",
       " 0.44928842782974243,\n",
       " 0.2833898067474365,\n",
       " 0.14990313351154327,\n",
       " 0.6357290744781494,\n",
       " 0.6620656251907349,\n",
       " 0.30660831928253174,\n",
       " 0.2519363462924957,\n",
       " 0.25342148542404175,\n",
       " 0.9125900268554688,\n",
       " 0.6798944473266602,\n",
       " 0.8253624439239502,\n",
       " 0.1201179176568985,\n",
       " 0.02341471053659916,\n",
       " 0.21843072772026062,\n",
       " 0.09163476526737213,\n",
       " -0.3944118916988373,\n",
       " -0.318728506565094,\n",
       " -0.229007288813591,\n",
       " 0.07624251395463943,\n",
       " -0.6142747402191162,\n",
       " 0.9264005422592163,\n",
       " -0.07648690044879913,\n",
       " -0.5341883301734924,\n",
       " 0.8691797256469727,\n",
       " 0.3377991318702698,\n",
       " 0.4671332538127899,\n",
       " 0.7296916246414185,\n",
       " -0.3750985264778137,\n",
       " -0.35146981477737427,\n",
       " -0.08778362721204758,\n",
       " -0.16267208755016327,\n",
       " 0.8767532110214233,\n",
       " 0.7074978351593018,\n",
       " -0.001486942172050476,\n",
       " 0.804984450340271,\n",
       " 0.6139711737632751,\n",
       " 0.6933713555335999,\n",
       " 0.7424333095550537,\n",
       " 0.5896860957145691,\n",
       " 0.992024302482605,\n",
       " 0.998259961605072,\n",
       " -0.5375128388404846,\n",
       " 0.9329419136047363,\n",
       " 0.9901387691497803,\n",
       " 0.2328946441411972,\n",
       " 0.6698432564735413,\n",
       " -0.5897128582000732,\n",
       " 0.9903843402862549,\n",
       " 0.9245737791061401,\n",
       " 0.6347125768661499,\n",
       " -0.556542158126831,\n",
       " 0.013530271127820015,\n",
       " 0.949970006942749,\n",
       " 0.6125165224075317,\n",
       " 0.5363390445709229,\n",
       " -0.29632633924484253,\n",
       " 0.916256308555603,\n",
       " -0.4670959413051605,\n",
       " -0.4515302777290344,\n",
       " 0.39485371112823486,\n",
       " 0.45443814992904663,\n",
       " 0.7951574325561523,\n",
       " 0.5089924931526184,\n",
       " 0.9570121169090271,\n",
       " -0.3987753093242645,\n",
       " -0.0886484682559967,\n",
       " 0.980345606803894,\n",
       " 0.9400767683982849,\n",
       " 0.9173567295074463,\n",
       " -0.2667890191078186,\n",
       " 0.8984326124191284,\n",
       " 0.16577282547950745,\n",
       " 0.6861518621444702,\n",
       " 0.003550032153725624,\n",
       " -0.03278495743870735,\n",
       " -0.5026047825813293,\n",
       " 0.9633495807647705,\n",
       " 0.9654731750488281,\n",
       " -0.5364320278167725,\n",
       " -0.18378661572933197,\n",
       " -0.3940388560295105,\n",
       " -0.18730783462524414,\n",
       " 0.8451758623123169,\n",
       " -0.3698977530002594,\n",
       " 0.5349951386451721,\n",
       " -0.5947994589805603,\n",
       " -0.534777045249939,\n",
       " 0.9617315530776978,\n",
       " 0.9918138384819031,\n",
       " 0.6689057350158691,\n",
       " -0.3462774455547333,\n",
       " -0.11011338233947754,\n",
       " 0.022125570103526115,\n",
       " 0.7664847373962402,\n",
       " 0.5066807270050049,\n",
       " -0.012835996225476265,\n",
       " -0.12228599190711975,\n",
       " -0.14113330841064453,\n",
       " -0.18929637968540192,\n",
       " -0.3971662223339081,\n",
       " 0.971744179725647,\n",
       " -0.5693280696868896,\n",
       " 0.863059937953949,\n",
       " 0.6576489210128784,\n",
       " 0.8295952081680298,\n",
       " 0.3232441544532776,\n",
       " 0.762734055519104,\n",
       " 0.938994824886322,\n",
       " -0.36332881450653076,\n",
       " 0.9859099388122559,\n",
       " 0.9760273694992065,\n",
       " 0.8137598037719727,\n",
       " -0.6577088832855225,\n",
       " -0.28148895502090454,\n",
       " -0.6036674976348877,\n",
       " 0.9704310894012451,\n",
       " 0.6284201145172119,\n",
       " 0.8552293181419373,\n",
       " 0.42720094323158264,\n",
       " -0.43812358379364014,\n",
       " -0.2127349078655243,\n",
       " 0.4900180697441101,\n",
       " -0.20284245908260345,\n",
       " 0.7987720370292664,\n",
       " 0.09436450153589249,\n",
       " 0.9069236516952515,\n",
       " -0.2449917197227478,\n",
       " 0.007583716884255409,\n",
       " 0.9925385117530823,\n",
       " -0.41770777106285095,\n",
       " 0.6526443958282471,\n",
       " -0.19621187448501587,\n",
       " 0.2626105844974518,\n",
       " -0.3600132465362549,\n",
       " -0.5372121930122375,\n",
       " -0.5399733781814575,\n",
       " 0.1750180870294571,\n",
       " 0.29307791590690613,\n",
       " 0.01962135173380375,\n",
       " -0.34762486815452576,\n",
       " -0.5542983412742615,\n",
       " -0.23338496685028076,\n",
       " 0.9708553552627563,\n",
       " 0.28995296359062195,\n",
       " 0.7463836073875427,\n",
       " 0.6524195671081543,\n",
       " 0.4028073847293854,\n",
       " 0.9749550819396973,\n",
       " -0.17794455587863922,\n",
       " -0.29275941848754883,\n",
       " 0.9395372867584229,\n",
       " -0.45971766114234924,\n",
       " 0.9921088218688965,\n",
       " -0.19592925906181335,\n",
       " -0.24666191637516022,\n",
       " -0.009060918353497982,\n",
       " -0.01799245923757553,\n",
       " -0.18348443508148193,\n",
       " 0.9895756840705872,\n",
       " 0.8285526037216187,\n",
       " 0.9507550001144409,\n",
       " 0.15486429631710052,\n",
       " 0.8275913000106812,\n",
       " -0.06707365065813065,\n",
       " -0.13160008192062378,\n",
       " -0.15703219175338745,\n",
       " 0.9215442538261414,\n",
       " 0.9966757297515869,\n",
       " -0.4233223795890808,\n",
       " -0.05968884378671646,\n",
       " -0.4041091203689575,\n",
       " 0.9906623363494873,\n",
       " -0.5315009951591492,\n",
       " 0.9691040515899658,\n",
       " -0.5363600254058838,\n",
       " 0.8352296352386475,\n",
       " -0.5114165544509888,\n",
       " 0.9858498573303223,\n",
       " -0.020474204793572426,\n",
       " 0.9453263282775879,\n",
       " -0.38505518436431885,\n",
       " 0.9070316553115845,\n",
       " -0.21922510862350464,\n",
       " 0.6593953371047974,\n",
       " -0.5574859380722046,\n",
       " 0.964667797088623,\n",
       " -0.04906080290675163,\n",
       " -0.49581050872802734,\n",
       " 0.9865683317184448,\n",
       " 0.14746353030204773,\n",
       " 0.971346378326416,\n",
       " 0.9706217050552368,\n",
       " -0.04827709496021271,\n",
       " 0.948936939239502,\n",
       " 0.9388283491134644,\n",
       " 0.9550765752792358,\n",
       " 0.9976924657821655,\n",
       " 0.9399541020393372,\n",
       " 0.20327228307724,\n",
       " 0.39648765325546265,\n",
       " 0.6310827732086182,\n",
       " 0.406249463558197,\n",
       " 0.45384854078292847,\n",
       " 0.16874395310878754,\n",
       " -0.025123825296759605,\n",
       " -0.15214884281158447,\n",
       " 0.748359203338623,\n",
       " 0.9814572334289551,\n",
       " 0.9613759517669678,\n",
       " 0.4115132987499237,\n",
       " 0.7051225900650024,\n",
       " 0.4443122446537018,\n",
       " -0.45729514956474304,\n",
       " 0.9636977314949036,\n",
       " -0.5272127389907837,\n",
       " 0.9169187545776367,\n",
       " 0.9486541748046875,\n",
       " 0.8803759217262268,\n",
       " 0.9784209728240967,\n",
       " -0.14761726558208466,\n",
       " 0.6400878429412842,\n",
       " 0.0455281063914299,\n",
       " 0.9572529196739197,\n",
       " 0.47602564096450806,\n",
       " 0.9004916548728943,\n",
       " -0.06974784284830093,\n",
       " 0.10991859436035156,\n",
       " -0.009979529306292534,\n",
       " 0.5703083872795105,\n",
       " 0.9759954810142517,\n",
       " 0.7844288349151611,\n",
       " -0.536189079284668,\n",
       " -0.5412949323654175,\n",
       " 0.9210329055786133,\n",
       " -0.06406847387552261,\n",
       " 0.9932478666305542,\n",
       " -0.5528243184089661,\n",
       " 0.9933939576148987,\n",
       " 0.9725452065467834,\n",
       " -0.571273684501648,\n",
       " 0.9947995543479919,\n",
       " -0.5527985095977783,\n",
       " -0.5770184993743896,\n",
       " 0.9742629528045654,\n",
       " -0.4091574251651764,\n",
       " -0.4932519793510437,\n",
       " 0.47400394082069397,\n",
       " 0.9197368621826172,\n",
       " -0.5560911893844604,\n",
       " 0.983474850654602,\n",
       " 0.9747126698493958,\n",
       " 0.6339210867881775,\n",
       " -0.15139535069465637,\n",
       " -0.16545924544334412,\n",
       " -0.4479405879974365,\n",
       " 0.7755067944526672,\n",
       " 0.04153698682785034,\n",
       " 0.8282428979873657,\n",
       " 0.8666592836380005,\n",
       " -0.6336268186569214,\n",
       " -0.4491628408432007,\n",
       " -0.6102538704872131,\n",
       " 0.9990244507789612,\n",
       " 0.9767605066299438,\n",
       " 0.8764538168907166,\n",
       " -0.5853369832038879,\n",
       " -0.5668882131576538,\n",
       " -0.23073171079158783,\n",
       " 0.9859027862548828,\n",
       " -0.568330705165863,\n",
       " 0.9557775259017944,\n",
       " 0.6529654860496521,\n",
       " -0.4244714379310608,\n",
       " 0.9391230344772339,\n",
       " -0.5495221614837646,\n",
       " 0.9483417272567749,\n",
       " -0.6079414486885071,\n",
       " 0.9423680305480957,\n",
       " 0.9516034722328186,\n",
       " 0.992233395576477,\n",
       " 0.5655796527862549,\n",
       " 0.832382082939148,\n",
       " -0.15186068415641785,\n",
       " 0.979918360710144,\n",
       " 0.9097530245780945,\n",
       " 0.9750666618347168,\n",
       " 0.9974930286407471,\n",
       " 0.08992093801498413,\n",
       " 0.4353247284889221,\n",
       " -0.567154049873352,\n",
       " -0.35770344734191895,\n",
       " 0.36599037051200867,\n",
       " 0.08810850977897644,\n",
       " 0.8283366560935974,\n",
       " 0.7468429803848267,\n",
       " 0.5696272850036621,\n",
       " -0.049425993114709854,\n",
       " 0.9108548164367676,\n",
       " 0.8359662294387817,\n",
       " 0.9390600919723511,\n",
       " -0.5952233672142029,\n",
       " 0.977151095867157,\n",
       " -0.5130683183670044,\n",
       " -0.5370741486549377,\n",
       " -0.6049132347106934,\n",
       " -0.2454620897769928,\n",
       " -0.597091555595398,\n",
       " 0.9651731252670288,\n",
       " -0.4707641899585724,\n",
       " 0.9624239802360535,\n",
       " 0.9988439083099365,\n",
       " -0.21316280961036682,\n",
       " -0.3213236331939697,\n",
       " -0.4745997190475464,\n",
       " 0.31039130687713623,\n",
       " -0.17431604862213135,\n",
       " -0.4991318881511688,\n",
       " 0.7310225963592529,\n",
       " -0.572791576385498,\n",
       " -0.14345303177833557,\n",
       " -0.059640295803546906,\n",
       " -0.3746105134487152,\n",
       " 0.8109028935432434,\n",
       " 0.4604886770248413,\n",
       " 0.9711315631866455,\n",
       " -0.27959075570106506,\n",
       " -0.45825499296188354,\n",
       " 0.9946731328964233,\n",
       " 0.9624844789505005,\n",
       " -0.5380646586418152,\n",
       " 0.9847425818443298,\n",
       " -0.5712786316871643,\n",
       " 0.02929576486349106,\n",
       " 0.9426369667053223,\n",
       " 0.6761425733566284,\n",
       " 0.9931126236915588,\n",
       " 0.9944230914115906,\n",
       " -0.4740196764469147,\n",
       " -0.5056855082511902,\n",
       " 0.9830892086029053,\n",
       " 0.9956314563751221,\n",
       " -0.42539089918136597,\n",
       " 0.9468915462493896,\n",
       " 0.958167552947998,\n",
       " -0.344958633184433,\n",
       " 0.16933928430080414,\n",
       " 0.9922580122947693,\n",
       " 0.7936270833015442,\n",
       " -0.5873197913169861,\n",
       " 0.43647676706314087,\n",
       " -0.6094498038291931,\n",
       " 0.9506727457046509,\n",
       " 0.9646739959716797,\n",
       " 0.9641684293746948,\n",
       " 0.9339423179626465,\n",
       " -0.601926326751709,\n",
       " 0.5442441701889038,\n",
       " 0.34955844283103943,\n",
       " -0.29590973258018494,\n",
       " -0.4376482367515564,\n",
       " -0.3244965374469757,\n",
       " 0.9533901810646057,\n",
       " 0.9580768942832947,\n",
       " 0.6882246732711792,\n",
       " -0.36717498302459717,\n",
       " 0.8918667435646057,\n",
       " -0.34438157081604004,\n",
       " 0.9279060363769531,\n",
       " 0.99498450756073,\n",
       " 0.08054449409246445,\n",
       " 0.9788967967033386,\n",
       " 0.9299111366271973,\n",
       " 0.8531751036643982,\n",
       " 0.9128881692886353,\n",
       " 0.8239109516143799,\n",
       " 0.7024112343788147,\n",
       " -0.3488287329673767,\n",
       " 0.977576732635498,\n",
       " 0.9880770444869995,\n",
       " -0.5356683731079102,\n",
       " 0.9551938772201538,\n",
       " 0.9027364253997803,\n",
       " 0.7375552654266357,\n",
       " 0.7022768259048462,\n",
       " 0.8608423471450806,\n",
       " 0.8881807327270508,\n",
       " 0.6203176975250244,\n",
       " 0.9161001443862915,\n",
       " 0.9815158843994141,\n",
       " -0.5822031497955322,\n",
       " 0.10649354010820389,\n",
       " 0.9865213632583618,\n",
       " 0.7634866237640381,\n",
       " -0.4710477590560913,\n",
       " 0.9708662629127502,\n",
       " -0.36230161786079407,\n",
       " 0.014376392588019371,\n",
       " 0.9791778922080994,\n",
       " 0.9551800489425659,\n",
       " -0.5750682353973389,\n",
       " -0.5074867010116577,\n",
       " -0.6086257696151733,\n",
       " -0.5676051378250122,\n",
       " 0.9409974217414856,\n",
       " 0.9929331541061401,\n",
       " 0.9955515265464783,\n",
       " -0.5689982175827026,\n",
       " 0.990572452545166,\n",
       " 0.22101446986198425,\n",
       " 0.6452433466911316,\n",
       " 0.957733154296875,\n",
       " -0.5717988014221191,\n",
       " 0.9383926391601562,\n",
       " 0.850792407989502,\n",
       " 0.939558744430542,\n",
       " 0.9415306448936462,\n",
       " -0.08993705362081528,\n",
       " -0.17339780926704407,\n",
       " -0.09934410452842712,\n",
       " 0.8026077151298523,\n",
       " 0.9966274499893188,\n",
       " 0.9789115190505981,\n",
       " 0.871646523475647,\n",
       " 0.059686146676540375,\n",
       " 0.9399234056472778,\n",
       " 0.5107926726341248,\n",
       " 0.7734986543655396,\n",
       " 0.8815014362335205,\n",
       " 0.4430346190929413,\n",
       " 0.9745190143585205,\n",
       " 0.8195434808731079,\n",
       " 0.9444389343261719,\n",
       " 0.8894557356834412,\n",
       " 0.9908270835876465,\n",
       " -0.10516133159399033,\n",
       " 0.7388068437576294,\n",
       " 0.039967119693756104,\n",
       " 0.2962157726287842,\n",
       " 0.9392966032028198,\n",
       " -0.4268537759780884,\n",
       " 0.9858108758926392,\n",
       " 0.30153051018714905,\n",
       " -0.569791853427887,\n",
       " 0.6253968477249146,\n",
       " 0.6718320846557617,\n",
       " 0.35476335883140564,\n",
       " 0.7562376260757446,\n",
       " 0.8326329588890076,\n",
       " 0.9463019967079163,\n",
       " -0.1520724594593048,\n",
       " 0.7928252816200256,\n",
       " -0.5757907032966614,\n",
       " 0.6220576763153076,\n",
       " -0.5847494006156921,\n",
       " 0.9822465181350708,\n",
       " 0.22335419058799744,\n",
       " 0.2865145206451416,\n",
       " 0.39940810203552246,\n",
       " 0.8195168972015381,\n",
       " -0.27377137541770935,\n",
       " -0.25078147649765015,\n",
       " -0.252336710691452,\n",
       " -0.3436465561389923,\n",
       " -0.5177912712097168,\n",
       " 0.8481672406196594,\n",
       " -0.49169600009918213,\n",
       " -0.03298037499189377,\n",
       " 0.8088709115982056,\n",
       " 0.8475306034088135,\n",
       " -0.1476050764322281,\n",
       " 0.26930367946624756,\n",
       " 0.5368850827217102,\n",
       " 0.7974026203155518,\n",
       " 0.513923168182373,\n",
       " -0.3606332540512085,\n",
       " 0.9027862548828125,\n",
       " 0.8164646029472351,\n",
       " -0.2132069170475006,\n",
       " -0.5806753039360046,\n",
       " -0.5805820822715759,\n",
       " -0.6064997911453247,\n",
       " 0.9969559907913208,\n",
       " 0.9955860376358032,\n",
       " 0.994179368019104,\n",
       " 0.24234676361083984,\n",
       " 0.922504723072052,\n",
       " 0.9987301826477051,\n",
       " 0.7970716953277588,\n",
       " 0.9491140842437744,\n",
       " 0.4135379195213318,\n",
       " 0.40958309173583984,\n",
       " 0.984654426574707,\n",
       " 0.618084192276001,\n",
       " -0.39999622106552124,\n",
       " 0.971397876739502,\n",
       " -0.547976553440094,\n",
       " 0.9814615249633789,\n",
       " 0.8375994563102722,\n",
       " 0.9552010893821716,\n",
       " 0.9420665502548218,\n",
       " 0.5161897540092468,\n",
       " 0.5736809968948364,\n",
       " 0.6233818531036377,\n",
       " -0.40233826637268066,\n",
       " 0.9774483442306519,\n",
       " 0.3917376399040222,\n",
       " 0.9584536552429199,\n",
       " 0.9949495792388916,\n",
       " -0.32193994522094727,\n",
       " 0.9586251974105835,\n",
       " -0.26689696311950684,\n",
       " -0.10887587070465088,\n",
       " -0.3224387466907501,\n",
       " 0.07458875328302383,\n",
       " 0.9742240905761719,\n",
       " 0.8562772274017334,\n",
       " 0.9706763029098511,\n",
       " -0.6081125736236572,\n",
       " -0.5065869688987732,\n",
       " 0.9893389940261841,\n",
       " 0.9901889562606812,\n",
       " -0.5347681045532227,\n",
       " 0.9415109157562256,\n",
       " -0.5901997685432434,\n",
       " -0.17371073365211487,\n",
       " 0.1583254337310791,\n",
       " -0.3354588747024536,\n",
       " -0.49306678771972656,\n",
       " -0.31671983003616333,\n",
       " 0.9744219779968262,\n",
       " -0.5765070915222168,\n",
       " 0.9095257520675659,\n",
       " 0.6081476211547852,\n",
       " 0.8380431532859802,\n",
       " 0.594789981842041,\n",
       " 0.21412257850170135,\n",
       " 0.8853297233581543,\n",
       " 0.5592092275619507,\n",
       " 0.7968077659606934,\n",
       " -0.4778714179992676,\n",
       " 0.980060338973999,\n",
       " -0.14870013296604156,\n",
       " 0.9924188852310181,\n",
       " -0.18018871545791626,\n",
       " -0.6013895273208618,\n",
       " -0.6195441484451294,\n",
       " -0.4359965920448303,\n",
       " 0.857276439666748,\n",
       " 0.5064249634742737,\n",
       " 0.5893182754516602,\n",
       " -0.20259138941764832,\n",
       " 0.8780998587608337,\n",
       " -0.1926172375679016,\n",
       " 0.902560830116272,\n",
       " 0.9780757427215576,\n",
       " 0.8846746683120728,\n",
       " 0.7189876437187195,\n",
       " 0.6888787746429443,\n",
       " 0.9754390716552734,\n",
       " 0.22082878649234772,\n",
       " 0.7309213280677795,\n",
       " 0.5284132957458496,\n",
       " 0.8145008683204651,\n",
       " -0.2404581904411316,\n",
       " -0.36675387620925903,\n",
       " 0.9559035301208496,\n",
       " 0.8056724071502686,\n",
       " 0.9337999224662781,\n",
       " -0.12346460670232773,\n",
       " -0.3435361981391907,\n",
       " -0.14837735891342163,\n",
       " -0.5501567125320435,\n",
       " -0.29318225383758545,\n",
       " 0.9992207288742065,\n",
       " -0.47032463550567627,\n",
       " 0.16220591962337494,\n",
       " -0.4842597544193268,\n",
       " 0.08385185152292252,\n",
       " -0.6078199148178101,\n",
       " 0.1450318694114685,\n",
       " 0.9736029505729675,\n",
       " 0.5458252429962158,\n",
       " 0.05437871813774109,\n",
       " 0.9701717495918274,\n",
       " -0.26491230726242065,\n",
       " -0.12143917381763458,\n",
       " 0.9954574704170227,\n",
       " -0.5397756099700928,\n",
       " -0.5874776840209961,\n",
       " 0.9900577068328857,\n",
       " -0.3931930959224701,\n",
       " 0.9644290208816528,\n",
       " -0.4041203260421753,\n",
       " -0.36250296235084534,\n",
       " -0.19820165634155273,\n",
       " 0.3398268222808838,\n",
       " 0.14482222497463226,\n",
       " 0.8103832602500916,\n",
       " -0.48801493644714355,\n",
       " 0.8283349275588989,\n",
       " 0.04784785211086273,\n",
       " -0.44869092106819153,\n",
       " 0.9784716963768005,\n",
       " 0.30989885330200195,\n",
       " 0.11192941665649414,\n",
       " 0.5490213632583618,\n",
       " 0.9216803908348083,\n",
       " -0.5510964393615723,\n",
       " -0.48364147543907166,\n",
       " -0.5479380488395691,\n",
       " 0.46149325370788574,\n",
       " 0.10161172598600388,\n",
       " 0.9838007688522339,\n",
       " -0.3623904585838318,\n",
       " -0.2688977122306824,\n",
       " 0.7741871476173401,\n",
       " -0.4211280941963196,\n",
       " -0.3321605920791626,\n",
       " 0.7068495750427246,\n",
       " 0.9765914678573608,\n",
       " 0.808048665523529,\n",
       " -0.5480388402938843,\n",
       " 0.80536288022995,\n",
       " 0.9443256855010986,\n",
       " 0.9958794116973877,\n",
       " -0.38623014092445374,\n",
       " 0.9043133854866028,\n",
       " 0.8663883805274963,\n",
       " 0.4616984724998474,\n",
       " 0.025008810684084892,\n",
       " -0.1903511881828308,\n",
       " -0.32159459590911865,\n",
       " -0.42913687229156494,\n",
       " 0.6269111633300781,\n",
       " 0.6498303413391113,\n",
       " -0.11556344479322433,\n",
       " -0.26819849014282227,\n",
       " 0.991327166557312,\n",
       " 0.4199478328227997,\n",
       " -0.04255545511841774,\n",
       " 0.5285751819610596,\n",
       " 0.9034157991409302,\n",
       " -0.3480226993560791,\n",
       " 0.30521517992019653,\n",
       " 0.5098332166671753,\n",
       " 0.39410701394081116,\n",
       " 0.4580914378166199,\n",
       " ...]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9157774448394775,\n",
       " 0.6625477075576782,\n",
       " 0.8609941005706787,\n",
       " 0.9316834211349487,\n",
       " 0.5915223360061646,\n",
       " 0.9719778895378113,\n",
       " 0.6087348461151123,\n",
       " 0.7313571572303772,\n",
       " 0.3838844299316406,\n",
       " 0.7630541324615479,\n",
       " 0.5186878442764282,\n",
       " 0.8974260091781616,\n",
       " 0.8886950016021729,\n",
       " 0.9138679504394531,\n",
       " 0.9384065866470337,\n",
       " 0.8894606828689575,\n",
       " 0.9953219890594482,\n",
       " 0.9666427969932556,\n",
       " 0.9743244647979736,\n",
       " 0.9606536626815796,\n",
       " 0.9797639846801758,\n",
       " 0.98703533411026,\n",
       " 0.991894006729126,\n",
       " 0.9925940632820129,\n",
       " 0.9916404485702515,\n",
       " 0.9577301740646362,\n",
       " 0.9522602558135986,\n",
       " 0.9263229370117188,\n",
       " 0.6953698396682739,\n",
       " 0.4797940254211426,\n",
       " 0.6873477697372437,\n",
       " 0.3773115277290344,\n",
       " 0.9072328805923462,\n",
       " 0.9390679597854614,\n",
       " 0.9475892782211304,\n",
       " 0.810563862323761,\n",
       " 0.8005375862121582,\n",
       " 0.9905440211296082,\n",
       " 0.9811105728149414,\n",
       " 0.9529976844787598,\n",
       " 0.9956005215644836,\n",
       " 0.9382787942886353,\n",
       " 0.942909300327301,\n",
       " 0.8194277286529541,\n",
       " 0.8193649053573608,\n",
       " 0.8761942386627197,\n",
       " 0.9303956627845764,\n",
       " 0.9288511276245117,\n",
       " 0.8207014799118042,\n",
       " 0.779084324836731,\n",
       " 0.9749484062194824,\n",
       " 0.8459869623184204,\n",
       " 0.9558035135269165,\n",
       " 0.8034095764160156,\n",
       " 0.9753419160842896,\n",
       " 0.35978156328201294,\n",
       " 0.6534902453422546,\n",
       " 0.6198400259017944,\n",
       " 0.594394862651825,\n",
       " 0.6836938261985779,\n",
       " 0.9712319374084473,\n",
       " 0.8370839357376099,\n",
       " 0.9316068887710571,\n",
       " 0.9452482461929321,\n",
       " 0.9940465688705444,\n",
       " 0.8894995450973511,\n",
       " 0.7171184420585632,\n",
       " 0.7058724164962769,\n",
       " 0.9138619899749756,\n",
       " 0.8461950421333313,\n",
       " 0.9303896427154541,\n",
       " 0.8090550899505615,\n",
       " 0.7739251852035522,\n",
       " 0.28771573305130005,\n",
       " 0.3894934058189392,\n",
       " 0.494294673204422,\n",
       " -0.12280768156051636,\n",
       " 0.9889203310012817,\n",
       " 0.9903395175933838,\n",
       " -0.289558082818985,\n",
       " 0.9817933440208435,\n",
       " 0.9965252876281738,\n",
       " 0.9781914949417114,\n",
       " 0.937180757522583,\n",
       " 0.8298150300979614,\n",
       " 0.9378514289855957,\n",
       " 0.9720730781555176,\n",
       " 0.9701571464538574,\n",
       " 0.8663426637649536,\n",
       " 0.5133892893791199,\n",
       " 0.9254910945892334,\n",
       " 0.8573325872421265,\n",
       " 0.9123263359069824,\n",
       " 0.6025434732437134,\n",
       " 0.9372368454933167,\n",
       " 0.28393447399139404,\n",
       " 0.9753355383872986,\n",
       " 0.9660645723342896,\n",
       " 0.9915614128112793,\n",
       " 0.9324206113815308,\n",
       " 0.9241305589675903,\n",
       " 0.8919194340705872,\n",
       " 0.9109653234481812,\n",
       " 0.8736511468887329,\n",
       " 0.9866214394569397,\n",
       " 0.8462439775466919,\n",
       " 0.9877935647964478,\n",
       " 0.955485463142395,\n",
       " 0.9188835620880127,\n",
       " 0.7278984785079956,\n",
       " 0.889984667301178,\n",
       " 0.9290025234222412,\n",
       " 0.9723836779594421,\n",
       " 0.9758265018463135,\n",
       " 0.9862556457519531,\n",
       " 0.3450760245323181,\n",
       " 0.9602117538452148,\n",
       " 0.9586094617843628,\n",
       " 0.915697455406189,\n",
       " 0.9836073517799377,\n",
       " 0.9606211185455322,\n",
       " 0.010478872805833817,\n",
       " 0.03844267874956131,\n",
       " 0.9297641515731812,\n",
       " 0.9558345079421997,\n",
       " 0.2447371482849121,\n",
       " 0.9672757387161255,\n",
       " 0.976142168045044,\n",
       " 0.9868142604827881,\n",
       " 0.9965911507606506,\n",
       " 0.9932330846786499,\n",
       " 0.9768948554992676,\n",
       " 0.9928559064865112,\n",
       " 0.9039735794067383,\n",
       " 0.8932657837867737,\n",
       " 0.8716564178466797,\n",
       " 0.9744157195091248,\n",
       " 0.9148329496383667,\n",
       " 0.5048409104347229,\n",
       " 0.5406656265258789,\n",
       " 0.6199989318847656,\n",
       " 0.9810372591018677,\n",
       " 0.8186680674552917,\n",
       " 0.9575061798095703,\n",
       " 0.794235110282898,\n",
       " 0.9236770868301392,\n",
       " 0.8922044038772583,\n",
       " 0.9909034967422485,\n",
       " 0.6245680451393127,\n",
       " 0.7289283275604248,\n",
       " 0.850247323513031,\n",
       " 0.844064474105835,\n",
       " 0.5293525457382202,\n",
       " 0.9473448991775513,\n",
       " 0.5621904134750366,\n",
       " 0.9698147773742676,\n",
       " 0.9867908954620361,\n",
       " 0.8768708109855652,\n",
       " 0.43173351883888245,\n",
       " 0.957088828086853,\n",
       " 0.8925334215164185,\n",
       " 0.7473546266555786,\n",
       " 0.8926730155944824,\n",
       " 0.7733036279678345,\n",
       " 0.7227926850318909,\n",
       " 0.9884488582611084,\n",
       " 0.9912122488021851,\n",
       " 0.9811105728149414,\n",
       " 0.3306984007358551,\n",
       " 0.9240506887435913,\n",
       " 0.994154691696167,\n",
       " 0.9407886266708374,\n",
       " 0.9675682187080383,\n",
       " 0.9819594025611877,\n",
       " 0.9822332859039307,\n",
       " 0.9448480606079102,\n",
       " 0.9580027461051941,\n",
       " 0.9838860034942627,\n",
       " 0.9156575202941895,\n",
       " 0.9654037952423096,\n",
       " 0.87977534532547,\n",
       " 0.9929212331771851,\n",
       " 0.9701917767524719,\n",
       " 0.8948241472244263,\n",
       " 0.4325060546398163,\n",
       " 0.8901165127754211,\n",
       " 0.9953624606132507,\n",
       " 0.9953818321228027,\n",
       " 0.2463875561952591,\n",
       " 0.8688504695892334,\n",
       " 0.9830938577651978,\n",
       " 0.8851995468139648,\n",
       " 0.9983846545219421,\n",
       " 0.4935428500175476,\n",
       " 0.9979798197746277,\n",
       " 0.9930758476257324,\n",
       " 0.9835132360458374,\n",
       " 0.9974972009658813,\n",
       " 0.980291485786438,\n",
       " 0.9591375589370728,\n",
       " 0.9441205859184265,\n",
       " 0.8245902061462402,\n",
       " 0.9386414885520935,\n",
       " 0.9700647592544556,\n",
       " 0.7601820230484009,\n",
       " 0.9759703874588013,\n",
       " 0.8846031427383423,\n",
       " 0.933860182762146,\n",
       " 0.6578987240791321,\n",
       " 0.8821442127227783,\n",
       " 0.9428443908691406,\n",
       " 0.8539799451828003,\n",
       " 0.43173351883888245,\n",
       " 0.35577070713043213,\n",
       " 0.8993697762489319,\n",
       " 0.38964974880218506,\n",
       " 0.8574707508087158,\n",
       " 0.9646815657615662,\n",
       " 0.8729578256607056,\n",
       " 0.7862898111343384,\n",
       " 0.904752254486084,\n",
       " 0.9654721617698669,\n",
       " 0.8994841575622559,\n",
       " 0.9436954259872437,\n",
       " 0.973703145980835,\n",
       " 0.542948842048645,\n",
       " 0.00844968855381012,\n",
       " 0.9822518229484558,\n",
       " 0.8714025020599365,\n",
       " 0.9940389394760132,\n",
       " 0.8175676465034485,\n",
       " 0.6523540616035461,\n",
       " 0.9939872622489929,\n",
       " 0.9864602088928223,\n",
       " 0.8583344221115112,\n",
       " 0.9993680119514465,\n",
       " 0.5552024841308594,\n",
       " 0.99184650182724,\n",
       " 0.9869163036346436,\n",
       " 0.9910870790481567,\n",
       " 0.9282214641571045,\n",
       " -0.0024838149547576904,\n",
       " 0.9154601097106934,\n",
       " 0.8978699445724487,\n",
       " 0.9840325713157654,\n",
       " 0.9978474378585815,\n",
       " 0.9961405992507935,\n",
       " 0.9871894121170044,\n",
       " 0.8193981051445007,\n",
       " 0.9808533191680908,\n",
       " 0.9817167520523071,\n",
       " 0.8388241529464722,\n",
       " 0.9879518747329712,\n",
       " 0.9575389623641968,\n",
       " 0.9925940632820129,\n",
       " 0.9209882616996765,\n",
       " 0.9788599610328674,\n",
       " 0.9595249891281128,\n",
       " 0.9836168885231018,\n",
       " 0.9891465902328491,\n",
       " 0.9975953102111816,\n",
       " 0.9612846374511719,\n",
       " 0.9382679462432861,\n",
       " 0.9643521904945374,\n",
       " 0.16662921011447906,\n",
       " 0.9124640226364136,\n",
       " 0.9596392512321472,\n",
       " 0.9470434784889221,\n",
       " 0.9228100776672363,\n",
       " 0.9775756001472473,\n",
       " 0.7209924459457397,\n",
       " 0.9784258008003235,\n",
       " 0.9663183689117432,\n",
       " 0.9119672179222107,\n",
       " 0.9947684407234192,\n",
       " 0.926557183265686,\n",
       " 0.8382511138916016,\n",
       " 0.9783110618591309,\n",
       " 0.954749345779419,\n",
       " 0.7763330936431885,\n",
       " 0.833319902420044,\n",
       " 0.9870228171348572,\n",
       " 0.883682370185852,\n",
       " 0.8995194435119629,\n",
       " 0.9614239931106567,\n",
       " 0.9654731750488281,\n",
       " 0.8056127429008484,\n",
       " 0.180558443069458,\n",
       " 0.9769285917282104,\n",
       " 0.9627034664154053,\n",
       " 0.8164765238761902,\n",
       " 0.8812968730926514,\n",
       " 0.8236311674118042,\n",
       " 0.9575389623641968,\n",
       " 0.9931761026382446,\n",
       " 0.9629209041595459,\n",
       " 0.9676282405853271,\n",
       " 0.985283613204956,\n",
       " 0.928546667098999,\n",
       " 0.5752092003822327,\n",
       " 0.7239195704460144,\n",
       " 0.7941535711288452,\n",
       " 0.2730656862258911,\n",
       " 0.19209527969360352,\n",
       " 0.990037202835083,\n",
       " 0.9733819365501404,\n",
       " 0.6687345504760742,\n",
       " 0.9328665733337402,\n",
       " 0.750503659248352,\n",
       " 0.7380753755569458,\n",
       " 0.006721539422869682,\n",
       " -0.18802626430988312,\n",
       " 0.8415165543556213,\n",
       " 0.13028784096240997,\n",
       " 0.9413712620735168,\n",
       " 0.9186968207359314,\n",
       " 0.9920670986175537,\n",
       " 0.6555465459823608,\n",
       " 0.4672791361808777,\n",
       " 0.951400637626648,\n",
       " 0.8601435422897339,\n",
       " 0.9883439540863037,\n",
       " 0.9358745813369751,\n",
       " 0.6958231329917908,\n",
       " -0.11883242428302765,\n",
       " 0.9913632869720459,\n",
       " 0.2615109086036682,\n",
       " 0.15479281544685364,\n",
       " 0.919441819190979,\n",
       " 0.8810074329376221,\n",
       " 0.8321741819381714,\n",
       " 0.8858343958854675,\n",
       " 0.8693849444389343,\n",
       " 0.512984037399292,\n",
       " 0.9827643632888794,\n",
       " 0.9893209934234619,\n",
       " -0.30375218391418457,\n",
       " 0.9992198348045349,\n",
       " 0.34630343317985535,\n",
       " 0.9946912527084351,\n",
       " -0.35487809777259827,\n",
       " 0.970264196395874,\n",
       " 0.9930009841918945,\n",
       " 0.9377226829528809,\n",
       " 0.6534902453422546,\n",
       " 0.845160722732544,\n",
       " 0.9780523777008057,\n",
       " 0.9483937621116638,\n",
       " 0.8882346153259277,\n",
       " 0.4749097228050232,\n",
       " 0.764553427696228,\n",
       " 0.41808396577835083,\n",
       " 0.21941271424293518,\n",
       " 0.26275795698165894,\n",
       " 0.3594564199447632,\n",
       " 0.24147340655326843,\n",
       " 0.8528004884719849,\n",
       " 0.8853592872619629,\n",
       " 0.9609929919242859,\n",
       " 0.9083134531974792,\n",
       " 0.8171464204788208,\n",
       " 0.6268979907035828,\n",
       " 0.8350869417190552,\n",
       " 0.7453092336654663,\n",
       " 0.9274402856826782,\n",
       " 0.7129217386245728,\n",
       " 0.8802439570426941,\n",
       " 0.8696401119232178,\n",
       " 0.9984948635101318,\n",
       " 0.9806748628616333,\n",
       " 0.9191451072692871,\n",
       " 0.9229527711868286,\n",
       " 0.9698524475097656,\n",
       " 0.220109760761261,\n",
       " 0.9806151390075684,\n",
       " 0.8232778310775757,\n",
       " 0.5701752305030823,\n",
       " 0.7531495094299316,\n",
       " 0.5273095369338989,\n",
       " 0.4451821744441986,\n",
       " 0.3332858383655548,\n",
       " 0.9754152297973633,\n",
       " 0.925132155418396,\n",
       " 0.9264494776725769,\n",
       " 0.3190622329711914,\n",
       " -0.045807529240846634,\n",
       " 0.09195905178785324,\n",
       " 0.08721008896827698,\n",
       " 0.4128316640853882,\n",
       " 0.9234048128128052,\n",
       " 0.7048027515411377,\n",
       " 0.6248345375061035,\n",
       " 0.9952742457389832,\n",
       " 0.9960477352142334,\n",
       " 0.05863676592707634,\n",
       " 0.9894740581512451,\n",
       " -0.033098842948675156,\n",
       " 0.9836475253105164,\n",
       " 0.887824535369873,\n",
       " 0.9563135504722595,\n",
       " 0.894763708114624,\n",
       " 0.9902201294898987,\n",
       " 0.9975584745407104,\n",
       " 0.6522401571273804,\n",
       " 0.9624320268630981,\n",
       " 0.9774215221405029,\n",
       " 0.4003443717956543,\n",
       " 0.8670696020126343,\n",
       " -0.3079545497894287,\n",
       " -0.10735692083835602,\n",
       " -0.18525587022304535,\n",
       " -0.30434495210647583,\n",
       " 0.7047680616378784,\n",
       " 0.9593462944030762,\n",
       " 0.7686835527420044,\n",
       " 0.7601107358932495,\n",
       " 0.802093505859375,\n",
       " 0.9487574100494385,\n",
       " 0.9879705905914307,\n",
       " 0.9800379276275635,\n",
       " 0.8151432275772095,\n",
       " 0.9421873092651367,\n",
       " 0.9251934885978699,\n",
       " 0.7324995994567871,\n",
       " 0.9718303084373474,\n",
       " 0.6396379470825195,\n",
       " 0.8829737305641174,\n",
       " 0.7199162840843201,\n",
       " 0.9910410642623901,\n",
       " 0.8096442818641663,\n",
       " 0.9960565567016602,\n",
       " 0.44023916125297546,\n",
       " 0.945259690284729,\n",
       " 0.9683657884597778,\n",
       " 0.4947759807109833,\n",
       " 0.9877293109893799,\n",
       " 0.8615168333053589,\n",
       " 0.9941492676734924,\n",
       " 0.9760606288909912,\n",
       " 0.9978641271591187,\n",
       " 0.47488537430763245,\n",
       " 0.9102873802185059,\n",
       " 0.9038515090942383,\n",
       " 0.6168115139007568,\n",
       " 0.9957010746002197,\n",
       " 0.8415271043777466,\n",
       " 0.9056315422058105,\n",
       " 0.8878303170204163,\n",
       " 0.9954822659492493,\n",
       " 0.9929713010787964,\n",
       " 0.9814196825027466,\n",
       " 0.8727169036865234,\n",
       " 0.9881804585456848,\n",
       " 0.9697323441505432,\n",
       " 0.9553678035736084,\n",
       " 0.9886467456817627,\n",
       " 0.7348089218139648,\n",
       " 0.6248773336410522,\n",
       " 0.9617928266525269,\n",
       " 0.8662424087524414,\n",
       " 0.9979000687599182,\n",
       " 0.9757178425788879,\n",
       " 0.9894203543663025,\n",
       " 0.6699843406677246,\n",
       " 0.9886104464530945,\n",
       " 0.984910249710083,\n",
       " 0.9384104013442993,\n",
       " 0.8747648000717163,\n",
       " 0.9829021096229553,\n",
       " 0.9565137624740601,\n",
       " 0.9679187536239624,\n",
       " 0.9108328819274902,\n",
       " 0.7370150685310364,\n",
       " 0.4706321358680725,\n",
       " 0.999085545539856,\n",
       " 0.7675749659538269,\n",
       " 0.9051672220230103,\n",
       " 0.8746833801269531,\n",
       " 0.863423764705658,\n",
       " 0.9235029220581055,\n",
       " 0.7848482131958008,\n",
       " 0.988340437412262,\n",
       " 0.36243677139282227,\n",
       " 0.9462645053863525,\n",
       " 0.9875613451004028,\n",
       " 0.9874752759933472,\n",
       " 0.9801759719848633,\n",
       " 0.9798763394355774,\n",
       " 0.9941470623016357,\n",
       " 0.9911902546882629,\n",
       " 0.9807426333427429,\n",
       " 0.9919891357421875,\n",
       " 0.9626755714416504,\n",
       " 0.980002224445343,\n",
       " 0.9168031811714172,\n",
       " 0.7470189929008484,\n",
       " 0.9362821578979492,\n",
       " 0.5017767548561096,\n",
       " 0.6993824243545532,\n",
       " 0.5198092460632324,\n",
       " 0.9821318984031677,\n",
       " 0.9875665903091431,\n",
       " 0.9286179542541504,\n",
       " 0.9439947009086609,\n",
       " 0.9397907257080078,\n",
       " 0.8190268874168396,\n",
       " 0.9782754182815552,\n",
       " 0.49812382459640503,\n",
       " 0.8544052243232727,\n",
       " 0.9681586027145386,\n",
       " 0.9322971105575562,\n",
       " 0.6840410232543945,\n",
       " 0.9209549427032471,\n",
       " 0.7587618231773376,\n",
       " 0.819846510887146,\n",
       " 0.7809023857116699,\n",
       " 0.955605149269104,\n",
       " 0.867142915725708,\n",
       " 0.8475602865219116,\n",
       " 0.8965439796447754,\n",
       " 0.7348089218139648,\n",
       " 0.9968444108963013,\n",
       " 0.9605276584625244,\n",
       " 0.9776821136474609,\n",
       " 0.9351508617401123,\n",
       " 0.9019089937210083,\n",
       " 0.9686161875724792,\n",
       " -0.24220064282417297,\n",
       " 0.9969195127487183,\n",
       " 0.8537185788154602,\n",
       " 0.9843531250953674,\n",
       " 0.9334515333175659,\n",
       " 0.5929402112960815,\n",
       " 0.975800633430481,\n",
       " 0.7047680616378784,\n",
       " 0.9887148141860962,\n",
       " 0.8838176727294922,\n",
       " 0.9421873092651367,\n",
       " 0.932908296585083,\n",
       " 0.7316257953643799,\n",
       " 0.9317506551742554,\n",
       " 0.950703501701355,\n",
       " 0.8447393774986267,\n",
       " 0.29141345620155334,\n",
       " 0.9926989078521729,\n",
       " 0.3523125946521759,\n",
       " 0.9828957319259644,\n",
       " 0.9874863624572754,\n",
       " 0.7298597097396851,\n",
       " 0.9583209753036499,\n",
       " 0.9889849424362183,\n",
       " 0.42826372385025024,\n",
       " 0.9438185691833496,\n",
       " 0.3958318829536438,\n",
       " 0.33443254232406616,\n",
       " 0.9626127481460571,\n",
       " 0.9656484127044678,\n",
       " 0.12363868951797485,\n",
       " 0.038525305688381195,\n",
       " 0.00844968855381012,\n",
       " 0.450508177280426,\n",
       " 0.9279541969299316,\n",
       " 0.9819506406784058,\n",
       " 0.9675459265708923,\n",
       " 0.9869831800460815,\n",
       " 0.9891735315322876,\n",
       " 0.9863604307174683,\n",
       " 0.952154278755188,\n",
       " 0.9433326125144958,\n",
       " 0.9405574202537537,\n",
       " 0.9650178551673889,\n",
       " 0.9469356536865234,\n",
       " 0.820770800113678,\n",
       " 0.8756029605865479,\n",
       " 0.8959548473358154,\n",
       " 0.8613011837005615,\n",
       " 0.8308607935905457,\n",
       " 0.8009644150733948,\n",
       " 0.8555895090103149,\n",
       " 0.8194706439971924,\n",
       " 0.982366681098938,\n",
       " 0.9776060581207275,\n",
       " 0.990447998046875,\n",
       " 0.9633324146270752,\n",
       " 0.9768955111503601,\n",
       " 0.6786751747131348,\n",
       " 0.9728731513023376,\n",
       " 0.4182680547237396,\n",
       " 0.9846899509429932,\n",
       " 0.925774097442627,\n",
       " 0.9883063435554504,\n",
       " 0.988799512386322,\n",
       " 0.0848618745803833,\n",
       " 0.9959861636161804,\n",
       " 0.9894776344299316,\n",
       " 0.9947968125343323,\n",
       " 0.994231641292572,\n",
       " 0.9580693244934082,\n",
       " 0.9734402894973755,\n",
       " 0.9846715927124023,\n",
       " 0.9369551539421082,\n",
       " 0.9396916031837463,\n",
       " 0.9822583794593811,\n",
       " 0.9953935742378235,\n",
       " 0.13331034779548645,\n",
       " 0.5406650900840759,\n",
       " 0.9582838416099548,\n",
       " 0.8908061981201172,\n",
       " 0.6719436645507812,\n",
       " 0.6204254627227783,\n",
       " 0.624497652053833,\n",
       " 0.6578254699707031,\n",
       " 0.9910356998443604,\n",
       " 0.9557172060012817,\n",
       " -0.16056418418884277,\n",
       " 0.9993101358413696,\n",
       " 0.9628991484642029,\n",
       " 0.9734757542610168,\n",
       " 0.9864753484725952,\n",
       " 0.9926214218139648,\n",
       " 0.9884294271469116,\n",
       " 0.9674193263053894,\n",
       " 0.9806989431381226,\n",
       " 0.9906433820724487,\n",
       " 0.9380618333816528,\n",
       " 0.990486741065979,\n",
       " 0.9862424731254578,\n",
       " 0.9749956130981445,\n",
       " 0.8341002464294434,\n",
       " 0.9369120597839355,\n",
       " 0.9985767006874084,\n",
       " 0.9954051971435547,\n",
       " 0.9883334636688232,\n",
       " 0.9670562148094177,\n",
       " 0.8931913375854492,\n",
       " 0.7853813767433167,\n",
       " 0.9931749105453491,\n",
       " 0.9129339456558228,\n",
       " 0.6062014102935791,\n",
       " 0.9929156303405762,\n",
       " 0.7930306792259216,\n",
       " 0.893667459487915,\n",
       " 0.8987842202186584,\n",
       " 0.7564409971237183,\n",
       " 0.059414997696876526,\n",
       " 0.9078476428985596,\n",
       " 0.015325888991355896,\n",
       " 0.1003728061914444,\n",
       " 0.19600632786750793,\n",
       " 0.9209321737289429,\n",
       " 0.7774122953414917,\n",
       " 0.9320346713066101,\n",
       " 0.9891815185546875,\n",
       " 0.9913007020950317,\n",
       " 0.9531205296516418,\n",
       " 0.7995212078094482,\n",
       " 0.9875448942184448,\n",
       " 0.9974389672279358,\n",
       " 0.9599823355674744,\n",
       " 0.9876114130020142,\n",
       " 0.9922906756401062,\n",
       " 0.9891496300697327,\n",
       " 0.9839402437210083,\n",
       " 0.9457155466079712,\n",
       " 0.9849410057067871,\n",
       " 0.9023535847663879,\n",
       " 0.975648045539856,\n",
       " 0.9661765098571777,\n",
       " -0.07236824929714203,\n",
       " 0.9665376543998718,\n",
       " 0.9854493141174316,\n",
       " 0.9734911918640137,\n",
       " 0.12302336096763611,\n",
       " 0.9391348361968994,\n",
       " 0.8840245604515076,\n",
       " 0.9688952565193176,\n",
       " 0.9704404473304749,\n",
       " -0.011594844982028008,\n",
       " 0.8649525046348572,\n",
       " 0.9701928496360779,\n",
       " 0.8821005821228027,\n",
       " 0.9778585433959961,\n",
       " 0.9854007363319397,\n",
       " 0.9956148862838745,\n",
       " 0.9984194040298462,\n",
       " 0.9543226361274719,\n",
       " 0.9923967719078064,\n",
       " 0.9928955435752869,\n",
       " 0.9593369960784912,\n",
       " 0.9456526041030884,\n",
       " 0.975236177444458,\n",
       " -0.09431377053260803,\n",
       " 0.9656805992126465,\n",
       " 0.9802078008651733,\n",
       " 0.9792023301124573,\n",
       " 0.9730890989303589,\n",
       " 0.9689669609069824,\n",
       " 0.9317434430122375,\n",
       " 0.997159481048584,\n",
       " 0.9967471361160278,\n",
       " 0.9678404331207275,\n",
       " 0.9951958060264587,\n",
       " 0.7347441911697388,\n",
       " 0.984338641166687,\n",
       " 0.9435542821884155,\n",
       " 0.8965855240821838,\n",
       " 0.9003024101257324,\n",
       " 0.9295933246612549,\n",
       " 0.9892708659172058,\n",
       " 0.8358516693115234,\n",
       " 0.956390380859375,\n",
       " 0.791046142578125,\n",
       " 0.719097375869751,\n",
       " 0.9762380719184875,\n",
       " 0.897970974445343,\n",
       " 0.91390061378479,\n",
       " 0.996309220790863,\n",
       " 0.995806097984314,\n",
       " 0.9824004769325256,\n",
       " 0.9664888381958008,\n",
       " 0.9678200483322144,\n",
       " 0.8752332925796509,\n",
       " 0.5248367786407471,\n",
       " 0.9948016405105591,\n",
       " 0.9202350378036499,\n",
       " 0.8870952725410461,\n",
       " 0.8851251006126404,\n",
       " 0.8670437932014465,\n",
       " 0.9777302742004395,\n",
       " 0.9914805889129639,\n",
       " 0.9457576870918274,\n",
       " 0.9976356029510498,\n",
       " 0.9791802167892456,\n",
       " 0.9934295415878296,\n",
       " 0.8630021810531616,\n",
       " 0.985796332359314,\n",
       " 0.9715487360954285,\n",
       " 0.931883692741394,\n",
       " 0.9533239006996155,\n",
       " 0.9470043182373047,\n",
       " 0.9668294191360474,\n",
       " 0.9854922294616699,\n",
       " 0.9873315095901489,\n",
       " 0.9379057288169861,\n",
       " 0.9466350078582764,\n",
       " 0.961084246635437,\n",
       " 0.9955689907073975,\n",
       " 0.9568443298339844,\n",
       " 0.9363054633140564,\n",
       " 0.9327846169471741,\n",
       " 0.9758726358413696,\n",
       " 0.983365535736084,\n",
       " 0.9949823021888733,\n",
       " 0.9950825572013855,\n",
       " 0.9930939674377441,\n",
       " 0.9772728681564331,\n",
       " 0.999273419380188,\n",
       " 0.9956148862838745,\n",
       " 0.9832642674446106,\n",
       " 0.9881176948547363,\n",
       " 0.9841488599777222,\n",
       " 0.9868613481521606,\n",
       " 0.9881685972213745,\n",
       " 0.8969762325286865,\n",
       " 0.9773443341255188,\n",
       " 0.9849035739898682,\n",
       " 0.9913302659988403,\n",
       " 0.9778211712837219,\n",
       " -0.11161451041698456,\n",
       " -0.12357689440250397,\n",
       " 0.9677908420562744,\n",
       " -0.17267000675201416,\n",
       " 0.5525467395782471,\n",
       " 0.46128618717193604,\n",
       " 0.39786413311958313,\n",
       " 0.4334836006164551,\n",
       " 0.00898478552699089,\n",
       " 0.13408713042736053,\n",
       " 0.13331034779548645,\n",
       " -0.028289280831813812,\n",
       " 0.9611667990684509,\n",
       " 0.9588742256164551,\n",
       " 0.9948792457580566,\n",
       " 0.9891969561576843,\n",
       " 0.9678865671157837,\n",
       " 0.9474990367889404,\n",
       " 0.92569899559021,\n",
       " 0.958099365234375,\n",
       " 0.9365575313568115,\n",
       " 0.8892472982406616,\n",
       " 0.8515084981918335,\n",
       " 0.9829285740852356,\n",
       " 0.0625925362110138,\n",
       " 0.9840415716171265,\n",
       " 0.9971182346343994,\n",
       " 0.8721928596496582,\n",
       " 0.35266536474227905,\n",
       " 0.4284903109073639,\n",
       " 0.4041822850704193,\n",
       " 0.40500861406326294,\n",
       " 0.957060694694519,\n",
       " 0.9669224619865417,\n",
       " 0.9509344696998596,\n",
       " 0.9396916031837463,\n",
       " 0.9897557497024536,\n",
       " 0.9758726358413696,\n",
       " 0.9950422644615173,\n",
       " 0.9758548140525818,\n",
       " 0.7518601417541504,\n",
       " 0.8087958097457886,\n",
       " 0.8089486360549927,\n",
       " 0.8146761655807495,\n",
       " 0.9734382629394531,\n",
       " 0.9285717010498047,\n",
       " 0.9268934726715088,\n",
       " 0.9356456995010376,\n",
       " 0.9798362851142883,\n",
       " 0.9648629426956177,\n",
       " 0.02140364982187748,\n",
       " 0.9454073905944824,\n",
       " 0.8093128204345703,\n",
       " 0.9941756725311279,\n",
       " 0.7943835258483887,\n",
       " 0.7991604804992676,\n",
       " 0.9982579946517944,\n",
       " 0.9931561946868896,\n",
       " 0.9463775753974915,\n",
       " 0.9635651707649231,\n",
       " 0.950141429901123,\n",
       " -0.14339417219161987,\n",
       " 0.9681541919708252,\n",
       " 0.984628438949585,\n",
       " 0.9862691164016724,\n",
       " 0.9544674754142761,\n",
       " 0.9864563941955566,\n",
       " 0.983402669429779,\n",
       " 0.9801375269889832,\n",
       " 0.956390380859375,\n",
       " 0.03837567940354347,\n",
       " 0.46208614110946655,\n",
       " 0.9878379106521606,\n",
       " 0.992073655128479,\n",
       " 0.9606940150260925,\n",
       " 0.989971935749054,\n",
       " -0.40091612935066223,\n",
       " 0.6410066485404968,\n",
       " 0.9962608814239502,\n",
       " 0.6032012104988098,\n",
       " -0.5152572989463806,\n",
       " -0.5673946142196655,\n",
       " -0.41485536098480225,\n",
       " -0.12900778651237488,\n",
       " 0.5376073718070984,\n",
       " 0.8702630400657654,\n",
       " 0.3162769377231598,\n",
       " 0.3232976198196411,\n",
       " 0.9844787120819092,\n",
       " 0.9607141017913818,\n",
       " 0.9555453658103943,\n",
       " 0.9207302927970886,\n",
       " 0.3410393297672272,\n",
       " 0.9409385919570923,\n",
       " -0.46308332681655884,\n",
       " -0.41183924674987793,\n",
       " 0.4128696918487549,\n",
       " 0.5625120997428894,\n",
       " -0.20381896197795868,\n",
       " -0.5054872035980225,\n",
       " 0.9806753396987915,\n",
       " 0.742828369140625,\n",
       " 0.23795664310455322,\n",
       " 0.26849499344825745,\n",
       " 0.9815295934677124,\n",
       " 0.9666894674301147,\n",
       " 0.9969683289527893,\n",
       " -0.5262627601623535,\n",
       " -0.4736706614494324,\n",
       " 0.9857128858566284,\n",
       " 0.8316479921340942,\n",
       " 0.28413820266723633,\n",
       " 0.6972661018371582,\n",
       " 0.9769868850708008,\n",
       " -0.5635676383972168,\n",
       " -0.5962754487991333,\n",
       " 0.46740007400512695,\n",
       " 0.5580692291259766,\n",
       " 0.35443177819252014,\n",
       " 0.38073769211769104,\n",
       " 0.32103902101516724,\n",
       " 0.6625316143035889,\n",
       " 0.6043534278869629,\n",
       " 0.6710920333862305,\n",
       " 0.9853332042694092,\n",
       " 0.988569438457489,\n",
       " -0.49174413084983826,\n",
       " -0.22916768491268158,\n",
       " -0.5322198271751404,\n",
       " 0.9914161562919617,\n",
       " 0.9914788007736206,\n",
       " -0.47856494784355164,\n",
       " 0.8763591051101685,\n",
       " 0.8999470472335815,\n",
       " -0.0946718230843544,\n",
       " 0.8156405687332153,\n",
       " -0.43008244037628174,\n",
       " 0.45243921875953674,\n",
       " 0.9929161071777344,\n",
       " 0.4720369279384613,\n",
       " 0.26343420147895813,\n",
       " 0.33573663234710693,\n",
       " 0.8512628078460693,\n",
       " 0.2638551890850067,\n",
       " 0.13022662699222565,\n",
       " 0.6999392509460449,\n",
       " 0.45349034667015076,\n",
       " 0.6519134640693665,\n",
       " -0.3917611539363861,\n",
       " -0.3156174123287201,\n",
       " -0.29431813955307007,\n",
       " 0.9385594725608826,\n",
       " 0.9688839912414551,\n",
       " 0.9277281165122986,\n",
       " 0.854755163192749,\n",
       " 0.9650062322616577,\n",
       " 0.3084694743156433,\n",
       " -0.5095982551574707,\n",
       " 0.977091908454895,\n",
       " -0.5633667707443237,\n",
       " 0.45707619190216064,\n",
       " 0.1979532241821289,\n",
       " 0.9992092847824097,\n",
       " 0.9067932963371277,\n",
       " 0.853955090045929,\n",
       " 0.07417663186788559,\n",
       " 0.9231612682342529,\n",
       " 0.7779372930526733,\n",
       " -0.24547217786312103,\n",
       " 0.5675772428512573,\n",
       " 0.8511455059051514,\n",
       " 0.82269287109375,\n",
       " 0.8963945508003235,\n",
       " 0.9818130135536194,\n",
       " 0.9914788007736206,\n",
       " -0.586223840713501,\n",
       " -0.5024880170822144,\n",
       " 0.14940015971660614,\n",
       " 0.7593819499015808,\n",
       " 0.17224812507629395,\n",
       " 0.9885795712471008,\n",
       " 0.9452743530273438,\n",
       " -0.5032479763031006,\n",
       " 0.9948659539222717,\n",
       " 0.9209111928939819,\n",
       " 0.9425323605537415,\n",
       " 0.3912901282310486,\n",
       " 0.9895085096359253,\n",
       " 0.7814286351203918,\n",
       " 0.5843965411186218,\n",
       " 0.8215731382369995,\n",
       " 0.7318031191825867,\n",
       " 0.9777861833572388,\n",
       " 0.8019669055938721,\n",
       " 0.22235232591629028,\n",
       " 0.9682081937789917,\n",
       " 0.6270290017127991,\n",
       " 0.3407769203186035,\n",
       " 0.9493693113327026,\n",
       " 0.3343901038169861,\n",
       " 0.8910396099090576,\n",
       " 0.8939118385314941,\n",
       " 0.9425323605537415,\n",
       " 0.33415400981903076,\n",
       " 0.2009979635477066,\n",
       " -0.5218518972396851,\n",
       " -0.3649464249610901,\n",
       " 0.9673234224319458,\n",
       " 0.8508846163749695,\n",
       " 0.7809683084487915,\n",
       " 0.974373996257782,\n",
       " 0.8589534163475037,\n",
       " 0.8637999296188354,\n",
       " -0.16878589987754822,\n",
       " 0.8381047248840332,\n",
       " 0.9355384111404419,\n",
       " 0.9278794527053833,\n",
       " 0.7814286351203918,\n",
       " 0.9654077887535095,\n",
       " 0.7470712661743164,\n",
       " 0.9307371377944946,\n",
       " 0.38742905855178833,\n",
       " -0.5938832759857178,\n",
       " 0.9163591861724854,\n",
       " 0.43728896975517273,\n",
       " 0.8973586559295654,\n",
       " -0.2571015954017639,\n",
       " 0.9196053743362427,\n",
       " 0.8109395503997803,\n",
       " 0.23658545315265656,\n",
       " 0.25426048040390015,\n",
       " 0.823717474937439,\n",
       " ...]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = trainer.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[\"model\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in c[\"model\"].items():\n",
    "    print(f\"{key} is {type(value)} with size {value.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in c[\"optimizer\"].items():\n",
    "    print(f\"{key} is {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in c[\"optimizer\"][\"state\"].items():\n",
    "    print(f\"{key} is {type(value)} with size {value.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[\"optimizer\"][\"state\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[\"settings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[\"step\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.valid_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.valid_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(params.valid_clients,'r') as trials:\n",
    "    scores = []\n",
    "    for line in trials:\n",
    "        sline = line[:-1].split()\n",
    "        print(\"-\"*50)\n",
    "        print(sline)\n",
    "        print(params.valid_data_dir  + sline[0] + '.pickle')\n",
    "        print(params.valid_data_dir + sline[1] + '.pickle')\n",
    "        print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.valid_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/notebooks/train/models/model1/CNN_VGG4L_3.5_128batchSize_0.0001lr_0.001weightDecay_1024kernel_400embSize_30.0s_0.4m_DoubleMHA_32_config.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = '/home/usuaris/veu/federico.costa/datasets/voxceleb2/dev/id09083/2CSYK-bJigI/00002.pickle'\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    x = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DASV",
   "language": "python",
   "name": "dasv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
