{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import getModelName, getNumberOfSpeakers\n",
    "from data import Dataset\n",
    "from model import SpeakerClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Input params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "execute and copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_data_dir': '/scratch/speaker_databases/', 'valid_data_dir': '/scratch/speaker_databases/VoxCeleb-1/wav', 'train_labels_path': 'labels/Vox2.ndx', 'data_mode': 'normal', 'valid_clients': 'labels/clients.ndx', 'valid_impostors': 'labels/impostors.ndx', 'out_dir': './models/model1', 'model_name': 'CNN', 'front_end': 'VGG4L', 'window_size': 3.5, 'randomSlicing': False, 'normalization': 'cmn', 'kernel_size': 1024, 'embedding_size': 400, 'heads_number': 32, 'pooling_method': 'DoubleMHA', 'mask_prob': 0.3, 'scalingFactor': 30.0, 'marginFactor': 0.4, 'annealing': False, 'optimizer': 'Adam', 'learning_rate': 0.0001, 'weight_decay': 0.001, 'batch_size': 64, 'gradientAccumulation': 2, 'max_epochs': 1000000, 'early_stopping': 25, 'print_every': 1000, 'requeue': False, 'validate_every': 10000, 'num_workers': 2}\r\n"
     ]
    }
   ],
   "source": [
    "! python args_input_simulation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "    'train_data_dir': '/scratch/speaker_databases/', \n",
    "    'valid_data_dir': '/scratch/speaker_databases/VoxCeleb-1/wav', \n",
    "    'train_labels_path': 'labels/Vox2.ndx', \n",
    "    'data_mode': 'normal', \n",
    "    'valid_clients': 'labels/clients.ndx', \n",
    "    'valid_impostors': 'labels/impostors.ndx', \n",
    "    'out_dir': './models/model1', \n",
    "    'model_name': 'CNN', \n",
    "    'front_end': 'VGG4L', \n",
    "    'window_size': 3.5, 'randomSlicing': False, \n",
    "    'normalization': 'cmn', \n",
    "    'kernel_size': 1024, \n",
    "    'embedding_size': 400, \n",
    "    'heads_number': 32, \n",
    "    'pooling_method': 'DoubleMHA', \n",
    "    'mask_prob': 0.3, \n",
    "    'scalingFactor': 30.0, \n",
    "    'marginFactor': 0.4, \n",
    "    'annealing': False, \n",
    "    'optimizer': 'Adam', \n",
    "    'learning_rate': 0.0001, \n",
    "    'weight_decay': 0.001, \n",
    "    'batch_size': 64, \n",
    "    'gradientAccumulation': 2, \n",
    "    'max_epochs': 1000000, \n",
    "    'early_stopping': 25, \n",
    "    'print_every': 1000, \n",
    "    'requeue': False, \n",
    "    'validate_every': 10000, \n",
    "    'num_workers': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5994 Speaker Labels\n",
      "Output folder created at ./models/model1\n",
      "./models/model1/CNN_VGG4L_3.5_128batchSize_0.0001lr_0.001weightDecay_1024kernel_400embSize_30.0s_0.4m_DoubleMHA_32_config.pkl\n"
     ]
    }
   ],
   "source": [
    "params = argparse.Namespace(**params_dict)\n",
    "\n",
    "params.model_name = getModelName(params)\n",
    "params.num_spkrs = getNumberOfSpeakers(params.train_labels_path) \n",
    "\n",
    "print(f\"{params.num_spkrs} Speaker Labels\")\n",
    "\n",
    "if not os.path.exists(params.out_dir):\n",
    "    os.makedirs(params.out_dir)\n",
    "print(f\"Output folder created at {params.out_dir}\")\n",
    "    \n",
    "with open(params.out_dir + '/' + params.model_name + '_config.pkl', 'wb') as handle:\n",
    "    pickle.dump(params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"{params.out_dir + '/' + params.model_name + '_config.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, params, device):\n",
    "\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "        self.__load_network()\n",
    "        self.__load_data()\n",
    "        self.__load_optimizer()\n",
    "        self.__load_criterion()\n",
    "        self.__initialize_training_variables()\n",
    "        \n",
    "        self.params.max_epochs = 2\n",
    "        \n",
    "        \n",
    "    def __load_data(self):\n",
    "        print('Loading Data and Labels...')\n",
    "        with open(self.params.train_labels_path, 'r') as data_labels_file:\n",
    "            print(f\"Getting train labels from {self.params.train_labels_path}...\")\n",
    "            train_labels = data_labels_file.readlines()\n",
    "\n",
    "        data_loader_parameters = {\n",
    "            'batch_size': self.params.batch_size, \n",
    "            'shuffle': True, \n",
    "            'num_workers': self.params.num_workers,\n",
    "        }\n",
    "        \n",
    "        self.training_generator = DataLoader(Dataset(train_labels, self.params), **data_loader_parameters)\n",
    "        \n",
    "        \n",
    "    def __load_network(self):\n",
    "\n",
    "        self.net = SpeakerClassifier(self.params, self.device)\n",
    "        self.net.to(self.device)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            self.net = nn.DataParallel(self.net)\n",
    "        \n",
    "    \n",
    "    def __load_optimizer(self):\n",
    "        if self.params.optimizer == 'Adam':\n",
    "            self.optimizer = optim.Adam(\n",
    "                self.net.parameters(), \n",
    "                lr=self.params.learning_rate, \n",
    "                weight_decay=self.params.weight_decay,\n",
    "            )\n",
    "        if self.params.optimizer == 'SGD':\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.net.parameters(), \n",
    "                lr=self.params.learning_rate, \n",
    "                weight_decay=self.params.weight_decay,\n",
    "            )\n",
    "        if self.params.optimizer == 'RMSprop':\n",
    "            self.optimizer = optim.RMSprop(\n",
    "                self.net.parameters(), \n",
    "                lr=self.params.learning_rate, \n",
    "                weight_decay=self.params.weight_decay,\n",
    "            )\n",
    "\n",
    "    \n",
    "    def __load_criterion(self):\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "    \n",
    "    def __load_previous_states(self):\n",
    "\n",
    "        list_files = os.listdir(self.params.out_dir)\n",
    "        list_files = [self.params.out_dir + '/' + f for f in list_files if '.chkpt' in f]\n",
    "        if list_files:\n",
    "            file2load = max(list_files, key=os.path.getctime)\n",
    "            checkpoint = torch.load(file2load, map_location=self.device)\n",
    "            try:\n",
    "                self.net.load_state_dict(checkpoint['model'])\n",
    "            except RuntimeError:\n",
    "                self.net.module.load_state_dict(checkpoint['model'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            self.params = checkpoint['settings']\n",
    "            self.starting_epoch = checkpoint['epoch']+1\n",
    "            self.step = checkpoint['step']\n",
    "            print('Model \"%s\" is Loaded for requeue process' % file2load)\n",
    "        else:\n",
    "            self.step = 0\n",
    "            self.starting_epoch = 1\n",
    "            \n",
    "    \n",
    "    def __initialize_training_variables(self):\n",
    "\n",
    "        if self.params.requeue:\n",
    "            self.__load_previous_states()\n",
    "        else:\n",
    "            self.step = 0\n",
    "            self.starting_epoch = 0\n",
    "\n",
    "        self.best_EER = 50.0\n",
    "        self.stopping = 0.0\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        print('Start Training')\n",
    "        for self.epoch in range(self.starting_epoch, self.params.max_epochs):  # loop over the dataset multiple times\n",
    "            print(f\"Epoch: {self.epoch}\")\n",
    "            self.net.train()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "    \n",
    "#print('Defining Device...')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f\"device: {device}\")\n",
    "#print(torch.cuda.get_device_name(0))\n",
    "\n",
    "trainer = Trainer(params, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DASV",
   "language": "python",
   "name": "dasv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
