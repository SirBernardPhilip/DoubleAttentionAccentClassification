{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train labels generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speakers_dict(load_path):\n",
    "    \n",
    "    speakers_set = set()\n",
    "    speakers_dict = {}\n",
    "    \n",
    "    for (dir_path, dir_names, file_names) in os.walk(load_path):\n",
    "        \n",
    "        # Directory should have some /id.../ part\n",
    "        speaker_chunk = [chunk for chunk in dir_path.split(\"/\") if chunk.startswith(\"id\")]\n",
    "    \n",
    "        # Only consider directories with /id.../\n",
    "        if len(speaker_chunk) > 0: \n",
    "        \n",
    "            speaker_id = speaker_chunk[0]\n",
    "            \n",
    "            # If speaker_id is looped for the first time, initialize variables\n",
    "            if speaker_id not in speakers_set:\n",
    "                speakers_dict[speaker_id] = {}\n",
    "                speakers_dict[speaker_id][\"files_paths\"] = set()\n",
    "                \n",
    "            # If it is a .pickle file, add the path to speakers_dict\n",
    "            for file_name in file_names:\n",
    "                if file_name.split(\".\")[-1] == \"pickle\":                \n",
    "                    file_path = dir_path + \"/\" + file_name.replace(\".pickle\", \"\")\n",
    "                    speakers_dict[speaker_id][\"files_paths\"].add(file_path)\n",
    "            \n",
    "            # Add speaker_id to set and continue with the loop\n",
    "            speakers_set.add(speaker_id)\n",
    "\n",
    "        # If there is some other \"/id...\" in the directory it should be looked\n",
    "        if len(speaker_chunk) > 1:\n",
    "            warnings.warn(f\"Ambiguous directory path: {dir_path}\")\n",
    "            \n",
    "    # Add 0 to total_speakers-1 labels\n",
    "    speakers_list = list(speakers_set)\n",
    "    speakers_list.sort()\n",
    "    \n",
    "    for i, speaker in enumerate(speakers_list):\n",
    "        speakers_dict[speaker][\"speaker_num\"] = i\n",
    "        \n",
    "    # Sort in order of labels for better understanding\n",
    "    speakers_dict = {k: v for k, v in sorted(speakers_dict.items(), key=lambda item: item[1][\"speaker_num\"])}\n",
    "    \n",
    "    return speakers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split_dict(speakers_dict, train_speakers_pctg, labels_dump_path, impostors_dump_path):\n",
    "    \n",
    "    # We are going to randomly split speaker_id's\n",
    "    \n",
    "    num_speakers = len(speakers_dict)\n",
    "    \n",
    "    train_speakers_final_index = int(num_speakers * train_speakers_pctg)\n",
    "    random_speaker_nums = list(range(num_speakers))\n",
    "    random.shuffle(random_speaker_nums)\n",
    "    \n",
    "    for i, speaker in enumerate(speakers_dict.keys()):\n",
    "        speakers_dict[speaker][\"random_speaker_num\"] = random_speaker_nums[i]\n",
    "        \n",
    "    train_speakers_dict = speakers_dict.copy()\n",
    "    valid_speakers_dict = speakers_dict.copy()\n",
    "\n",
    "    for speaker in speakers_dict.keys():\n",
    "\n",
    "        random_speaker_num = speakers_dict[speaker][\"random_speaker_num\"]\n",
    "\n",
    "        if random_speaker_num > train_speakers_final_index:\n",
    "            del train_speakers_dict[speaker]\n",
    "        else:\n",
    "            del valid_speakers_dict[speaker]\n",
    "\n",
    "    train_speakers_num = len(train_speakers_dict.keys())\n",
    "    valid_speakers_num = len(valid_speakers_dict.keys())\n",
    "    total_speakers_num = train_speakers_num + valid_speakers_num\n",
    "    if total_speakers_num != num_speakers:\n",
    "        raise Exception(\"total_speakers_num does not match total_speakers_num!\")\n",
    "    train_speakers_pctg = train_speakers_num * 100 / total_speakers_num\n",
    "    valid_speakers_pctg = valid_speakers_num * 100 / total_speakers_num\n",
    "    \n",
    "    \n",
    "    train_files_num = len(list(itertools.chain.from_iterable([value[\"files_paths\"] for value in train_speakers_dict.values()])))\n",
    "    valid_files_num = len(list(itertools.chain.from_iterable([value[\"files_paths\"] for value in valid_speakers_dict.values()])))\n",
    "    total_files_num = train_files_num + valid_files_num\n",
    "    train_files_pctg = train_files_num * 100 / total_files_num\n",
    "    valid_files_pctg = valid_files_num * 100 / total_files_num\n",
    "    \n",
    "    \n",
    "    print(f\"{train_speakers_num} speakers ({train_speakers_pctg:.1f}%) with a total of {train_files_num} files ({train_files_pctg:.1f}%) in training split.\")\n",
    "    print(f\"{valid_speakers_num} speakers ({valid_speakers_pctg:.1f}%) with a total of {valid_files_num} files ({valid_files_pctg:.1f}%) in training split.\")\n",
    "    \n",
    "    return train_speakers_dict, valid_speakers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels_file(dump_path, speakers_dict):\n",
    "    \n",
    "    with open(dump_path, 'w') as f:\n",
    "        for key, value in speakers_dict.items():\n",
    "            speaker_num = value[\"speaker_num\"]\n",
    "            for file_path in value[\"files_paths\"]:\n",
    "                line_to_write = f\"{file_path} {speaker_num} -1\"  \n",
    "                f.write(line_to_write)\n",
    "                f.write('\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clients_impostors_files(\n",
    "    impostors_dump_path, clients_dump_path, \n",
    "    speakers_dict, \n",
    "    clients_lines_max = None, impostors_lines_max = None):\n",
    "    \n",
    "    clients_lines_to_write = []\n",
    "    impostors_lines_to_write = []\n",
    "\n",
    "    distinct_speakers = list(speakers_dict.keys())\n",
    "\n",
    "    one_speaker_combinations = [(speaker, speaker) for speaker in distinct_speakers]\n",
    "    two_speaker_combinations = list(itertools.combinations(distinct_speakers, 2))  \n",
    "    speaker_combinations = one_speaker_combinations + two_speaker_combinations\n",
    "\n",
    "    for speaker_1, speaker_2 in speaker_combinations:\n",
    "\n",
    "        speaker_1_files = speakers_dict[speaker_1][\"files_paths\"]\n",
    "        speaker_2_files = speakers_dict[speaker_2][\"files_paths\"]\n",
    "\n",
    "        if speaker_1 == speaker_2:\n",
    "            files_combinations = list(itertools.combinations(speaker_1_files, 2))\n",
    "            for file_1, file_2 in files_combinations:\n",
    "                line_to_write = file_1 + \" \" + file_2\n",
    "                clients_lines_to_write.append(line_to_write)\n",
    "        else:\n",
    "            files_combinations = list(itertools.product(speaker_1_files, speaker_2_files))\n",
    "            for file_1, file_2 in files_combinations:\n",
    "                line_to_write = file_1 + \" \" + file_2\n",
    "                impostors_lines_to_write.append(line_to_write)\n",
    "\n",
    "    if clients_lines_max is not None:\n",
    "        clients_lines_to_write = random.sample(clients_lines_to_write, clients_lines_max)\n",
    "    if impostors_lines_max is not None:\n",
    "        impostors_lines_to_write = random.sample(impostors_lines_to_write, impostors_lines_max)\n",
    "    \n",
    "    print(f\"{len(clients_lines_to_write)} lines to write for clients.\")\n",
    "    print(f\"{len(impostors_lines_to_write)} lines to write for impostors.\")\n",
    "    \n",
    "    with open(clients_dump_path, 'w') as f:\n",
    "        for line_to_write in clients_lines_to_write: \n",
    "            f.write(line_to_write)\n",
    "            f.write('\\n')\n",
    "        f.close()\n",
    "\n",
    "    with open(impostors_dump_path, 'w') as f:\n",
    "        for line_to_write in impostors_lines_to_write: \n",
    "            f.write(line_to_write)\n",
    "            f.write('\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11336/2890324868.py:32: UserWarning: Ambiguous directory path: /home/usuaris/veu/federico.costa/datasets/voxceleb2/dev/id09210/ido93Up7-ig\n",
      "  warnings.warn(f\"Ambiguous directory path: {dir_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of distinct speakers loaded: 101\n",
      "Spliting data into train and valid...\n",
      "81 speakers (80.2%) with a total of 12290 files (76.6%) in training split.\n",
      "20 speakers (19.8%) with a total of 3748 files (23.4%) in training split.\n",
      "Data splited.\n",
      "Generating training labels...\n",
      "Training labels generated.\n",
      "Generating valid clients and impostors trials...\n",
      "526836 lines to write for clients.\n",
      "6495042 lines to write for impostors.\n",
      "GValid clients and impostors trials generated.\n"
     ]
    }
   ],
   "source": [
    "# Setted variables\n",
    "dev_dataset_path = \"/home/usuaris/veu/federico.costa/datasets/voxceleb2/dev\"\n",
    "train_speakers_pctg = 0.8\n",
    "labels_dump_path = \"/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/files_directories/labels/labels.ndx\"\n",
    "impostors_dump_path = \"/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/files_directories/labels/impostors.ndx\"\n",
    "clients_dump_path = \"/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/files_directories/labels/clients.ndx\"\n",
    "\n",
    "print(\"Loading dev data...\")\n",
    "dev_speakers_dict = generate_speakers_dict(\n",
    "    load_path = dev_dataset_path,\n",
    ")\n",
    "\n",
    "num_speakers = len(dev_speakers_dict)\n",
    "print(f\"Total number of distinct speakers loaded: {num_speakers}\")\n",
    "\n",
    "print(f\"Spliting data into train and valid...\")\n",
    "train_speakers_dict, valid_speakers_dict = train_valid_split_dict(\n",
    "    speakers_dict = dev_speakers_dict, \n",
    "    train_speakers_pctg = train_speakers_pctg, \n",
    "    labels_dump_path = None, \n",
    "    impostors_dump_path = None,\n",
    ")\n",
    "print(f\"Data splited.\")\n",
    "\n",
    "print(f\"Generating training labels...\")\n",
    "generate_labels_file(\n",
    "    dump_path = labels_dump_path, \n",
    "    speakers_dict = train_speakers_dict,\n",
    ")\n",
    "print(f\"Training labels generated.\")\n",
    "\n",
    "print(f\"Generating valid clients and impostors trials...\")\n",
    "generate_clients_impostors_files(\n",
    "    impostors_dump_path = impostors_dump_path, \n",
    "    clients_dump_path = clients_dump_path, \n",
    "    speakers_dict = valid_speakers_dict, \n",
    "    clients_lines_max = None, \n",
    "    impostors_lines_max = None,\n",
    ")\n",
    "print(f\"Valid clients and impostors trials generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DASV",
   "language": "python",
   "name": "dasv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
